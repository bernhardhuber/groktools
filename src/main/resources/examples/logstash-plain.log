[2020-01-30T22:26:57,775][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration"}
[2020-01-30T22:26:57,822][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration"}
[2020-01-30T22:27:00,256][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
[2020-01-30T22:27:00,991][ERROR][logstash.pipeline        ] Error registering plugin {:plugin=>"<LogStash::Inputs::File path=>[\"test.log\"], start_position=>\"beginning\", codec=><LogStash::Codecs::Multiline pattern=>\"^%{TIMESTAMP_ISO8601}\", negate=>true, what=>\"previous\", id=>\"5214ce8cd6b5e57d516944cb1e2b93fb03be178c-1\", enable_metric=>true, charset=>\"UTF-8\", multiline_tag=>\"multiline\", max_lines=>500, max_bytes=>10485760>, id=>\"5214ce8cd6b5e57d516944cb1e2b93fb03be178c-2\", enable_metric=>true, stat_interval=>1, discover_interval=>15, sincedb_write_interval=>15, delimiter=>\"\\n\", close_older=>3600>", :error=>"File paths must be absolute, relative path specified: test.log"}
[2020-01-30T22:27:01,116][ERROR][logstash.agent           ] Pipeline aborted due to error {:exception=>#<ArgumentError: File paths must be absolute, relative path specified: test.log>, :backtrace=>["D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-input-file-4.0.3/lib/logstash/inputs/file.rb:187:in `register'", "org/jruby/RubyArray.java:1613:in `each'", "D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-input-file-4.0.3/lib/logstash/inputs/file.rb:185:in `register'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:290:in `register_plugin'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:301:in `register_plugins'", "org/jruby/RubyArray.java:1613:in `each'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:301:in `register_plugins'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:456:in `start_inputs'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:348:in `start_workers'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:235:in `run'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/agent.rb:408:in `start_pipeline'"]}
[2020-01-30T22:27:01,337][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-01-30T22:27:04,142][WARN ][logstash.agent           ] stopping pipeline {:id=>"main"}
[2020-01-30T22:27:52,263][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration"}
[2020-01-30T22:27:52,279][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"fb_apache", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0xf060454 @kibana_version_parts=["5", "6", "0"], @module_name="fb_apache", @directory="D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration">}
[2020-01-30T22:27:52,279][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration"}
[2020-01-30T22:27:52,279][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"netflow", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x7d97f04b @kibana_version_parts=["5", "6", "0"], @module_name="netflow", @directory="D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration">}
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] -------- Logstash Settings (* means modified) ---------
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] node.name: "xxxx"
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] *path.config: "confs\\_xxxx2-lo4j.conf"
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] path.data: "D:/projects/elkstack/logstash-5.6.4/data"
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] modules.cli: []
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] modules: []
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] modules_setup: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] config.test_and_exit: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] config.reload.automatic: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] config.support_escapes: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] config.reload.interval: 3
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] metric.collect: true
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] pipeline.id: "main"
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] pipeline.system: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] pipeline.workers: 4
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] pipeline.output.workers: 1
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] pipeline.batch.size: 125
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] pipeline.batch.delay: 5
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] pipeline.unsafe_shutdown: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] path.plugins: []
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] config.debug: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] *log.level: "debug" (default: "info")
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] version: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] help: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] log.format: "plain"
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] http.host: "127.0.0.1"
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] http.port: 9600..9700
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] http.environment: "production"
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] queue.type: "memory"
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] queue.drain: false
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] queue.page_capacity: 262144000
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] queue.max_bytes: 1073741824
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] queue.max_events: 0
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] queue.checkpoint.acks: 1024
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] queue.checkpoint.writes: 1024
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] queue.checkpoint.interval: 1000
[2020-01-30T22:27:52,294][DEBUG][logstash.runner          ] dead_letter_queue.enable: false
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] dead_letter_queue.max_bytes: 1073741824
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] slowlog.threshold.warn: -1
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] slowlog.threshold.info: -1
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] slowlog.threshold.debug: -1
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] slowlog.threshold.trace: -1
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] path.queue: "D:/projects/elkstack/logstash-5.6.4/data/queue"
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] path.dead_letter_queue: "D:/projects/elkstack/logstash-5.6.4/data/dead_letter_queue"
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] path.settings: "D:/projects/elkstack/logstash-5.6.4/config"
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] path.logs: "D:/projects/elkstack/logstash-5.6.4/logs"
[2020-01-30T22:27:52,310][DEBUG][logstash.runner          ] --------------- Logstash Settings -------------------
[2020-01-30T22:27:52,326][DEBUG][logstash.agent           ] Agent: Configuring metric collection
[2020-01-30T22:27:52,326][DEBUG][logstash.instrument.periodicpoller.os] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:27:52,341][DEBUG][logstash.instrument.periodicpoller.jvm] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:27:52,372][DEBUG][logstash.instrument.periodicpoller.persistentqueue] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:27:52,372][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:27:52,372][DEBUG][logstash.agent           ] Reading config file {:config_file=>"D:/projects/elkstack/logstash-5.6.4/confs/_xxxx2-lo4j.conf"}
[2020-01-30T22:27:52,482][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"multiline", :type=>"codec", :class=>LogStash::Codecs::Multiline}
[2020-01-30T22:27:52,482][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@pattern = "^%{TIMESTAMP_ISO8601}"
[2020-01-30T22:27:52,482][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@negate = true
[2020-01-30T22:27:52,497][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@what = "previous"
[2020-01-30T22:27:52,497][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@id = "5214ce8cd6b5e57d516944cb1e2b93fb03be178c-1"
[2020-01-30T22:27:52,497][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@enable_metric = true
[2020-01-30T22:27:52,497][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@patterns_dir = []
[2020-01-30T22:27:52,497][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@charset = "UTF-8"
[2020-01-30T22:27:52,497][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@multiline_tag = "multiline"
[2020-01-30T22:27:52,497][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_lines = 500
[2020-01-30T22:27:52,497][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_bytes = 10485760
[2020-01-30T22:27:52,830][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/aws"}
[2020-01-30T22:27:52,846][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bacula"}
[2020-01-30T22:27:52,846][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bind"}
[2020-01-30T22:27:52,846][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bro"}
[2020-01-30T22:27:52,846][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/exim"}
[2020-01-30T22:27:52,846][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/firewalls"}
[2020-01-30T22:27:52,846][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns"}
[2020-01-30T22:27:52,861][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/haproxy"}
[2020-01-30T22:27:52,861][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/httpd"}
[2020-01-30T22:27:52,861][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/java"}
[2020-01-30T22:27:52,861][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/junos"}
[2020-01-30T22:27:52,877][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/linux-syslog"}
[2020-01-30T22:27:52,877][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/maven"}
[2020-01-30T22:27:52,877][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective"}
[2020-01-30T22:27:52,877][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective-patterns"}
[2020-01-30T22:27:52,877][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mongodb"}
[2020-01-30T22:27:52,877][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/nagios"}
[2020-01-30T22:27:52,877][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/postgresql"}
[2020-01-30T22:27:52,877][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/rails"}
[2020-01-30T22:27:52,893][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/redis"}
[2020-01-30T22:27:52,893][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/ruby"}
[2020-01-30T22:27:52,893][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/squid"}
[2020-01-30T22:27:52,955][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"file", :type=>"input", :class=>LogStash::Inputs::File}
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@pattern = "^%{TIMESTAMP_ISO8601}"
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@negate = true
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@what = "previous"
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@id = "5214ce8cd6b5e57d516944cb1e2b93fb03be178c-1"
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@enable_metric = true
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@patterns_dir = []
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@charset = "UTF-8"
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@multiline_tag = "multiline"
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_lines = 500
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_bytes = 10485760
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/aws"}
[2020-01-30T22:27:52,955][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bacula"}
[2020-01-30T22:27:52,971][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bind"}
[2020-01-30T22:27:52,971][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bro"}
[2020-01-30T22:27:52,971][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/exim"}
[2020-01-30T22:27:52,971][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/firewalls"}
[2020-01-30T22:27:52,971][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns"}
[2020-01-30T22:27:52,971][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/haproxy"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/httpd"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/java"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/junos"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/linux-syslog"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/maven"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective-patterns"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mongodb"}
[2020-01-30T22:27:52,986][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/nagios"}
[2020-01-30T22:27:53,002][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/postgresql"}
[2020-01-30T22:27:53,002][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/rails"}
[2020-01-30T22:27:53,002][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/redis"}
[2020-01-30T22:27:53,002][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/ruby"}
[2020-01-30T22:27:53,002][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/squid"}
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@path = ["test.log"]
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@start_position = "beginning"
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@codec = <LogStash::Codecs::Multiline pattern=>"^%{TIMESTAMP_ISO8601}", negate=>true, what=>"previous", id=>"5214ce8cd6b5e57d516944cb1e2b93fb03be178c-1", enable_metric=>true, charset=>"UTF-8", multiline_tag=>"multiline", max_lines=>500, max_bytes=>10485760>
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@id = "5214ce8cd6b5e57d516944cb1e2b93fb03be178c-2"
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@enable_metric = true
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@add_field = {}
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@stat_interval = 1
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@discover_interval = 15
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@sincedb_write_interval = 15
[2020-01-30T22:27:53,018][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@delimiter = "\n"
[2020-01-30T22:27:53,033][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@close_older = 3600
[2020-01-30T22:27:53,033][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"mutate", :type=>"filter", :class=>LogStash::Filters::Mutate}
[2020-01-30T22:27:53,049][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@gsub = ["message", "r", ""]
[2020-01-30T22:27:53,049][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@id = "5214ce8cd6b5e57d516944cb1e2b93fb03be178c-3"
[2020-01-30T22:27:53,049][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@enable_metric = true
[2020-01-30T22:27:53,049][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@add_tag = []
[2020-01-30T22:27:53,049][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@remove_tag = []
[2020-01-30T22:27:53,049][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@add_field = {}
[2020-01-30T22:27:53,049][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@remove_field = []
[2020-01-30T22:27:53,049][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@periodic_flush = false
[2020-01-30T22:27:53,096][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"grok", :type=>"filter", :class=>LogStash::Filters::Grok}
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@match = {"message"=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@overwrite = ["message"]
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@id = "5214ce8cd6b5e57d516944cb1e2b93fb03be178c-4"
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@enable_metric = true
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_tag = []
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_tag = []
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_field = {}
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_field = []
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@periodic_flush = false
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_dir = []
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@pattern_definitions = {}
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_files_glob = "*"
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@break_on_match = true
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@named_captures_only = true
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@keep_empty_captures = false
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_failure = ["_grokparsefailure"]
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_millis = 30000
[2020-01-30T22:27:53,096][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_timeout = "_groktimeout"
[2020-01-30T22:27:53,127][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"date", :type=>"filter", :class=>LogStash::Filters::Date}
[2020-01-30T22:27:53,140][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@match = ["timestamp", "yyyy-MM-dd HH:mm:ss,SSS"]
[2020-01-30T22:27:53,141][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@id = "5214ce8cd6b5e57d516944cb1e2b93fb03be178c-5"
[2020-01-30T22:27:53,141][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@enable_metric = true
[2020-01-30T22:27:53,142][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@add_tag = []
[2020-01-30T22:27:53,142][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@remove_tag = []
[2020-01-30T22:27:53,142][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@add_field = {}
[2020-01-30T22:27:53,143][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@remove_field = []
[2020-01-30T22:27:53,143][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@periodic_flush = false
[2020-01-30T22:27:53,144][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@target = "@timestamp"
[2020-01-30T22:27:53,144][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@tag_on_failure = ["_dateparsefailure"]
[2020-01-30T22:27:53,158][DEBUG][org.logstash.filters.DateFilter] Date filter with format=yyyy-MM-dd HH:mm:ss,SSS, locale=null, timezone=null built as org.logstash.filters.parser.JodaParser
[2020-01-30T22:27:53,158][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"stdout", :type=>"output", :class=>LogStash::Outputs::Stdout}
[2020-01-30T22:27:53,189][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"rubydebug", :type=>"codec", :class=>LogStash::Codecs::RubyDebug}
[2020-01-30T22:27:53,189][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@id = "rubydebug_25116b4f-41ba-4b1b-a432-287210e5874d"
[2020-01-30T22:27:53,189][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@enable_metric = true
[2020-01-30T22:27:53,189][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@metadata = false
[2020-01-30T22:27:53,517][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@codec = <LogStash::Codecs::RubyDebug id=>"rubydebug_25116b4f-41ba-4b1b-a432-287210e5874d", enable_metric=>true, metadata=>false>
[2020-01-30T22:27:53,517][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@id = "5214ce8cd6b5e57d516944cb1e2b93fb03be178c-6"
[2020-01-30T22:27:53,517][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@enable_metric = true
[2020-01-30T22:27:53,517][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@workers = 1
[2020-01-30T22:27:53,517][DEBUG][logstash.agent           ] starting agent
[2020-01-30T22:27:53,517][DEBUG][logstash.agent           ] starting pipeline {:id=>"main"}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Grok patterns path {:paths=>["D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns", "D:/projects/elkstack/logstash-5.6.4/patterns/*"]}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Grok patterns path {:paths=>[]}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Match data {:match=>{"message"=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] regexp: /message {:pattern=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Adding pattern {"S3_REQUEST_LINE"=>"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Adding pattern {"S3_ACCESS_LOG"=>"%{WORD:owner} %{NOTSPACE:bucket} \\[%{HTTPDATE:timestamp}\\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:\"%{S3_REQUEST_LINE}\"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:\"?%{QS:agent}\"?|-) (?:-|%{NOTSPACE:version_id})"}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_URIPATHPARAM"=>"%{URIPATH:path}(?:%{URIPARAM:params})?"}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_URI"=>"%{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?"}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_REQUEST_LINE"=>"(?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-01-30T22:27:53,533][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_ACCESS_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} \"%{ELB_REQUEST_LINE}\""}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"CLOUDFRONT_ACCESS_LOG"=>"(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}\\t%{TIME})\\t%{WORD:x_edge_location}\\t(?:%{NUMBER:sc_bytes:int}|-)\\t%{IPORHOST:clientip}\\t%{WORD:cs_method}\\t%{HOSTNAME:cs_host}\\t%{NOTSPACE:cs_uri_stem}\\t%{NUMBER:sc_status:int}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:agent}\\t%{GREEDYDATA:cs_uri_query}\\t%{GREEDYDATA:cookies}\\t%{WORD:x_edge_result_type}\\t%{NOTSPACE:x_edge_request_id}\\t%{HOSTNAME:x_host_header}\\t%{URIPROTO:cs_protocol}\\t%{INT:cs_bytes:int}\\t%{GREEDYDATA:time_taken:float}\\t%{GREEDYDATA:x_forwarded_for}\\t%{GREEDYDATA:ssl_protocol}\\t%{GREEDYDATA:ssl_cipher}\\t%{GREEDYDATA:x_edge_response_result_type}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_TIMESTAMP"=>"%{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_HOST"=>"[a-zA-Z0-9-]+"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_VOLUME"=>"%{USER}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_DEVICE"=>"%{USER}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_DEVICEPATH"=>"%{UNIXPATH}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_CAPACITY"=>"%{INT}{1,3}(,%{INT}{3})*"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_VERSION"=>"%{USER}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_JOB"=>"%{USER}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MAX_CAPACITY"=>"User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_END_VOLUME"=>"End of medium on Volume \\\"%{BACULA_VOLUME:volume}\\\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_VOLUME"=>"Created new Volume \\\"%{BACULA_VOLUME:volume}\\\" in catalog."}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_LABEL"=>"Labeled new Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)."}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_WROTE_LABEL"=>"Wrote label to prelabeled Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_MOUNT"=>"New volume \\\"%{BACULA_VOLUME:volume}\\\" mounted on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOOPEN"=>"\\s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOOPENDIR"=>"\\s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOSTAT"=>"\\s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:27:53,549][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOJOBS"=>"There are no more Jobs associated with Volume \\\"%{BACULA_VOLUME:volume}\\\". Marking it purged."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_ALL_RECORDS_PRUNED"=>"All records pruned from Volume \\\"%{BACULA_VOLUME:volume}\\\"; marking it \\\"Purged\\\""}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_JOBS"=>"Begin pruning Jobs older than %{INT} month %{INT} days ."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_FILES"=>"Begin pruning Files."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_PRUNED_JOBS"=>"Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_PRUNED_FILES"=>"Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_ENDPRUNE"=>"End auto prune."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_STARTJOB"=>"Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}"}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_STARTRESTORE"=>"Start Restore Job %{BACULA_JOB:job}"}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_USEDEVICE"=>"Using Device \\\"%{BACULA_DEVICE:device}\\\""}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_DIFF_FS"=>"\\s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_JOBEND"=>"Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second"}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRUNE_JOBS"=>"No Jobs found to prune."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRUNE_FILES"=>"No Files found to prune."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_VOLUME_PREVWRITTEN"=>"Volume \\\"%{BACULA_VOLUME:volume}\\\" previously written, moving to end of data."}
[2020-01-30T22:27:53,564][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_READYAPPEND"=>"Ready to append to end of Volume \\\"%{BACULA_VOLUME:volume}\\\" size=%{INT}"}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_CANCELLING"=>"Cancelling duplicate JobId=%{INT}."}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MARKCANCEL"=>"JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled."}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_CLIENT_RBJ"=>"shell command: run ClientRunBeforeJob \\\"%{GREEDYDATA:runjob}\\\""}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_VSS"=>"(Generate )?VSS (Writer)?"}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MAXSTART"=>"Fatal error: Job canceled because max start delay time exceeded."}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_DUPLICATE"=>"Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed."}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOJOBSTAT"=>"Fatal error: No Job status returned from FD."}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_FATAL_CONN"=>"Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NO_CONNECT"=>"Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NO_AUTH"=>"Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:"}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOSUIT"=>"No prior or suitable Full backup found in catalog. Doing FULL backup."}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRIOR"=>"No prior Full backup Job record found."}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_JOB"=>"(Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \\(%{BACULA_VERSION}\\):"}
[2020-01-30T22:27:53,580][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOGLINE"=>"%{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})"}
[2020-01-30T22:27:53,596][DEBUG][logstash.filters.grok    ] Adding pattern {"BIND9_TIMESTAMP"=>"%{MONTHDAY}[-]%{MONTH}[-]%{YEAR} %{TIME}"}
[2020-01-30T22:27:53,596][DEBUG][logstash.filters.grok    ] Adding pattern {"BIND9"=>"%{BIND9_TIMESTAMP:timestamp} queries: %{LOGLEVEL:loglevel}: client %{IP:clientip}#%{POSINT:clientport} \\(%{GREEDYDATA:query}\\): query: %{GREEDYDATA:query} IN %{GREEDYDATA:querytype} \\(%{IP:dns}\\)"}
[2020-01-30T22:27:53,596][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_HTTP"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{INT:trans_depth}\\t%{GREEDYDATA:method}\\t%{GREEDYDATA:domain}\\t%{GREEDYDATA:uri}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:user_agent}\\t%{NUMBER:request_body_len}\\t%{NUMBER:response_body_len}\\t%{GREEDYDATA:status_code}\\t%{GREEDYDATA:status_msg}\\t%{GREEDYDATA:info_code}\\t%{GREEDYDATA:info_msg}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:bro_tags}\\t%{GREEDYDATA:username}\\t%{GREEDYDATA:password}\\t%{GREEDYDATA:proxied}\\t%{GREEDYDATA:orig_fuids}\\t%{GREEDYDATA:orig_mime_types}\\t%{GREEDYDATA:resp_fuids}\\t%{GREEDYDATA:resp_mime_types}"}
[2020-01-30T22:27:53,596][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_DNS"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{INT:trans_id}\\t%{GREEDYDATA:query}\\t%{GREEDYDATA:qclass}\\t%{GREEDYDATA:qclass_name}\\t%{GREEDYDATA:qtype}\\t%{GREEDYDATA:qtype_name}\\t%{GREEDYDATA:rcode}\\t%{GREEDYDATA:rcode_name}\\t%{GREEDYDATA:AA}\\t%{GREEDYDATA:TC}\\t%{GREEDYDATA:RD}\\t%{GREEDYDATA:RA}\\t%{GREEDYDATA:Z}\\t%{GREEDYDATA:answers}\\t%{GREEDYDATA:TTLs}\\t%{GREEDYDATA:rejected}"}
[2020-01-30T22:27:53,596][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_CONN"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{GREEDYDATA:service}\\t%{NUMBER:duration}\\t%{NUMBER:orig_bytes}\\t%{NUMBER:resp_bytes}\\t%{GREEDYDATA:conn_state}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:missed_bytes}\\t%{GREEDYDATA:history}\\t%{GREEDYDATA:orig_pkts}\\t%{GREEDYDATA:orig_ip_bytes}\\t%{GREEDYDATA:resp_pkts}\\t%{GREEDYDATA:resp_ip_bytes}\\t%{GREEDYDATA:tunnel_parents}"}
[2020-01-30T22:27:53,596][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_FILES"=>"%{NUMBER:ts}\\t%{NOTSPACE:fuid}\\t%{IP:tx_hosts}\\t%{IP:rx_hosts}\\t%{NOTSPACE:conn_uids}\\t%{GREEDYDATA:source}\\t%{GREEDYDATA:depth}\\t%{GREEDYDATA:analyzers}\\t%{GREEDYDATA:mime_type}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:duration}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:is_orig}\\t%{GREEDYDATA:seen_bytes}\\t%{GREEDYDATA:total_bytes}\\t%{GREEDYDATA:missing_bytes}\\t%{GREEDYDATA:overflow_bytes}\\t%{GREEDYDATA:timedout}\\t%{GREEDYDATA:parent_fuid}\\t%{GREEDYDATA:md5}\\t%{GREEDYDATA:sha1}\\t%{GREEDYDATA:sha256}\\t%{GREEDYDATA:extracted}"}
[2020-01-30T22:27:53,596][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_MSGID"=>"[0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_FLAGS"=>"(<=|[-=>*]>|[*]{2}|==)"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_DATE"=>"%{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_PID"=>"\\[%{POSINT}\\]"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_QT"=>"((\\d+y)?(\\d+w)?(\\d+d)?(\\d+h)?(\\d+m)?(\\d+s)?)"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_EXCLUDE_TERMS"=>"(Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_REMOTE_HOST"=>"(H=(%{NOTSPACE:remote_hostname} )?(\\(%{NOTSPACE:remote_heloname}\\) )?\\[%{IP:remote_host}\\])"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_INTERFACE"=>"(I=\\[%{IP:exim_interface}\\](:%{NUMBER:exim_interface_port}))"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_PROTOCOL"=>"(P=%{NOTSPACE:protocol})"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_MSG_SIZE"=>"(S=%{NUMBER:exim_msg_size})"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_HEADER_ID"=>"(id=%{NOTSPACE:exim_header_id})"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_SUBJECT"=>"(T=%{QS:exim_subject})"}
[2020-01-30T22:27:53,611][DEBUG][logstash.filters.grok    ] Adding pattern {"NETSCREENSESSIONLOG"=>"%{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_TAGGED_SYSLOG"=>"^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOTIMESTAMP"=>"%{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOTAG"=>"[A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_ACTION"=>"Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_REASON"=>"Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\\s*)*"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_DIRECTION"=>"Inbound|inbound|Outbound|outbound"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_INTERVAL"=>"first hit|%{INT}-second interval"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_XLATE_TYPE"=>"static|dynamic"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104001"=>"\\((?:Primary|Secondary)\\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104002"=>"\\((?:Primary|Secondary)\\) Switching to STANDBY - %{GREEDYDATA:switch_reason}"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104003"=>"\\((?:Primary|Secondary)\\) Switching to FAILED\\."}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104004"=>"\\((?:Primary|Secondary)\\) Switching to OK\\."}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105003"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105004"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal"}
[2020-01-30T22:27:53,627][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105005"=>"\\((?:Primary|Secondary)\\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-01-30T22:27:53,646][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105008"=>"\\((?:Primary|Secondary)\\) Testing [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-01-30T22:27:53,647][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105009"=>"\\((?:Primary|Secondary)\\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)"}
[2020-01-30T22:27:53,647][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106001"=>"%{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:27:53,648][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106006_106007_106010"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\\(%{DATA:src_fwuser}\\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\\(%{DATA:dst_fwuser}\\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})"}
[2020-01-30T22:27:53,649][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106014"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\\(%{DATA:dst_fwuser}\\))? \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\)"}
[2020-01-30T22:27:53,650][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106015"=>"%{CISCO_ACTION:action} %{WORD:protocol} \\(%{DATA:policy_id}\\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:27:53,651][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106021"=>"%{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:27:53,652][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106023"=>"%{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\\(%{DATA:dst_fwuser}\\))?( \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\))? by access-group \"?%{DATA:policy_id}\"? \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:27:53,653][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106100_2_3"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\) -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:27:53,654][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106100"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\)(\\(%{DATA:src_fwuser}\\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\)(\\(%{DATA:src_fwuser}\\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:27:53,655][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW304001"=>"%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? Accessed URL %{IP:dst_ip}:%{GREEDYDATA:dst_url}"}
[2020-01-30T22:27:53,655][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW110002"=>"%{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:27:53,656][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302010"=>"%{INT:connection_count} in use, %{INT:connection_count_max} most used"}
[2020-01-30T22:27:53,656][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302013_302014_302015_302016"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \\(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\\))?(\\(%{DATA:src_fwuser}\\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \\(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\\))?(\\(%{DATA:dst_fwuser}\\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \\(%{DATA:user}\\))?"}
[2020-01-30T22:27:53,658][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302020_302021"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\\(%{DATA:fwuser}\\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \\(%{DATA:user}\\))?"}
[2020-01-30T22:27:53,658][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW305011"=>"%{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}"}
[2020-01-30T22:27:53,659][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW313001_313004_313008"=>"%{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?"}
[2020-01-30T22:27:53,660][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW313005"=>"%{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\\(%{DATA:err_src_fwuser}\\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\\(%{DATA:err_dst_fwuser}\\))? \\(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\\) on %{DATA:interface} interface\\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\\(%{DATA:orig_src_fwuser}\\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\\(%{DATA:orig_dst_fwuser}\\))?"}
[2020-01-30T22:27:53,662][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW321001"=>"Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system"}
[2020-01-30T22:27:53,662][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW402117"=>"%{WORD:protocol}: Received a non-IPSec packet \\(protocol= %{WORD:orig_protocol}\\) from %{IP:src_ip} to %{IP:dst_ip}"}
[2020-01-30T22:27:53,663][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW402119"=>"%{WORD:protocol}: Received an %{WORD:orig_protocol} packet \\(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\\) from %{IP:src_ip} \\(user= %{DATA:user}\\) to %{IP:dst_ip} that failed anti-replay checking"}
[2020-01-30T22:27:53,663][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW419001"=>"%{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}"}
[2020-01-30T22:27:53,664][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW419002"=>"%{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number"}
[2020-01-30T22:27:53,665][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW500004"=>"%{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:27:53,665][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW602303_602304"=>"%{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \\(SPI= %{DATA:spi}\\) between %{IP:src_ip} and %{IP:dst_ip} \\(user= %{DATA:user}\\) has been %{CISCO_ACTION:action}"}
[2020-01-30T22:27:53,666][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW710001_710002_710003_710005_710006"=>"%{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:27:53,666][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW713172"=>"Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\\s+Remote end\\s*%{DATA:is_remote_natted}\\s*behind a NAT device\\s+This\\s+end\\s*%{DATA:is_local_natted}\\s*behind a NAT device"}
[2020-01-30T22:27:53,667][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW733100"=>"\\[\\s*%{DATA:drop_type}\\s*\\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}"}
[2020-01-30T22:27:53,668][DEBUG][logstash.filters.grok    ] Adding pattern {"SHOREWALL"=>"(%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)"}
[2020-01-30T22:27:53,669][DEBUG][logstash.filters.grok    ] Adding pattern {"SFW2"=>"((%{SYSLOGTIMESTAMP})|(%{TIMESTAMP_ISO8601}))\\s*%{HOSTNAME}\\s*kernel\\S+\\s*%{NAGIOSTIME}\\s*SFW2\\-INext\\-%{NOTSPACE:nf_action}\\s*IN=%{USERNAME:nf_in_interface}.*OUT=((\\s*%{USERNAME:nf_out_interface})|(\\s*))MAC=((%{COMMONMAC:nf_dst_mac}:%{COMMONMAC:nf_src_mac})|(\\s*)).*SRC=%{IP:nf_src_ip}\\s*DST=%{IP:nf_dst_ip}.*PROTO=%{WORD:nf_protocol}((.*SPT=%{INT:nf_src_port}.*DPT=%{INT:nf_dst_port}.*)|())"}
[2020-01-30T22:27:53,672][DEBUG][logstash.filters.grok    ] Adding pattern {"USERNAME"=>"[a-zA-Z0-9._-]+"}
[2020-01-30T22:27:53,673][DEBUG][logstash.filters.grok    ] Adding pattern {"USER"=>"%{USERNAME}"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"EMAILLOCALPART"=>"[a-zA-Z][a-zA-Z0-9_.+-=:]+"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"EMAILADDRESS"=>"%{EMAILLOCALPART}@%{HOSTNAME}"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"INT"=>"(?:[+-]?(?:[0-9]+))"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE10NUM"=>"(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"NUMBER"=>"(?:%{BASE10NUM})"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE16NUM"=>"(?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE16FLOAT"=>"\\b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"POSINT"=>"\\b(?:[1-9][0-9]*)\\b"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"NONNEGINT"=>"\\b(?:[0-9]+)\\b"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"WORD"=>"\\b\\w+\\b"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"NOTSPACE"=>"\\S+"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"SPACE"=>"\\s*"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"DATA"=>".*?"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"GREEDYDATA"=>".*"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"QUOTEDSTRING"=>"(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"UUID"=>"[A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"URN"=>"urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"MAC"=>"(?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOMAC"=>"(?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4})"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"WINDOWSMAC"=>"(?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"COMMONMAC"=>"(?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"IPV6"=>"((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"IPV4"=>"(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"IP"=>"(?:%{IPV6}|%{IPV4})"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"HOSTNAME"=>"\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)"}
[2020-01-30T22:27:53,674][DEBUG][logstash.filters.grok    ] Adding pattern {"IPORHOST"=>"(?:%{IP}|%{HOSTNAME})"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"HOSTPORT"=>"%{IPORHOST}:%{POSINT}"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"PATH"=>"(?:%{UNIXPATH}|%{WINPATH})"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"UNIXPATH"=>"(/([\\w_%!$@:.,+~-]+|\\\\.)*)+"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"TTY"=>"(?:/dev/(pts|tty([pq])?)(\\w+)?/?(?:[0-9]+))"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"WINPATH"=>"(?>[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPROTO"=>"[A-Za-z]([A-Za-z0-9+\\-.]+)+"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"URIHOST"=>"%{IPORHOST}(?::%{POSINT:port})?"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPATH"=>"(?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\\-]*)+"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPARAM"=>"\\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]<>]*"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPATHPARAM"=>"%{URIPATH}(?:%{URIPARAM})?"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"URI"=>"%{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTH"=>"\\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|ä)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHNUM"=>"(?:0?[1-9]|1[0-2])"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHNUM2"=>"(?:0[1-9]|1[0-2])"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHDAY"=>"(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"DAY"=>"(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"YEAR"=>"(?>\\d\\d){1,2}"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"HOUR"=>"(?:2[0123]|[01]?[0-9])"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"MINUTE"=>"(?:[0-5][0-9])"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"SECOND"=>"(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"TIME"=>"(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE_US"=>"%{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE_EU"=>"%{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"ISO8601_TIMEZONE"=>"(?:Z|[+-]%{HOUR}(?::?%{MINUTE}))"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"ISO8601_SECOND"=>"(?:%{SECOND}|60)"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"TIMESTAMP_ISO8601"=>"%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE"=>"%{DATE_US}|%{DATE_EU}"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP"=>"%{DATE}[- ]%{TIME}"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"TZ"=>"(?:[APMCE][SD]T|UTC)"}
[2020-01-30T22:27:53,690][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_RFC822"=>"%{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_RFC2822"=>"%{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_OTHER"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_EVENTLOG"=>"%{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGTIMESTAMP"=>"%{MONTH} +%{MONTHDAY} %{TIME}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"PROG"=>"[\\x21-\\x5a\\x5c\\x5e-\\x7e]+"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGPROG"=>"%{PROG:program}(?:\\[%{POSINT:pid}\\])?"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGHOST"=>"%{IPORHOST}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGFACILITY"=>"<%{NONNEGINT:facility}.%{NONNEGINT:priority}>"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDATE"=>"%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"QS"=>"%{QUOTEDSTRING}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGBASE"=>"%{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"LOGLEVEL"=>"([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYTIME"=>"(?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYDATE"=>"%{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYCAPTUREDREQUESTHEADERS"=>"%{DATA:captured_request_headers}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYCAPTUREDRESPONSEHEADERS"=>"%{DATA:captured_response_headers}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYHTTPBASE"=>"%{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\\{%{HAPROXYCAPTUREDREQUESTHEADERS}\\})?( )?(\\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\\})?( )?\"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?\""}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYHTTP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"}
[2020-01-30T22:27:53,705][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYTCP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDUSER"=>"%{EMAILADDRESS}|%{USER}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDERROR_DATE"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_COMMONLOG"=>"%{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_COMBINEDLOG"=>"%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD20_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{LOGLEVEL:loglevel}\\] (?:\\[client %{IPORHOST:clientip}\\] ){0,1}%{GREEDYDATA:message}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD24_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{WORD:module}:%{LOGLEVEL:loglevel}\\] \\[pid %{POSINT:pid}(:tid %{NUMBER:tid})?\\]( \\(%{POSINT:proxy_errorcode}\\)%{DATA:proxy_message}:)?( \\[client %{IPORHOST:clientip}:%{POSINT:clientport}\\])?( %{DATA:errorcode}:)? %{GREEDYDATA:message}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_ERRORLOG"=>"%{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"COMMONAPACHELOG"=>"%{HTTPD_COMMONLOG}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"COMBINEDAPACHELOG"=>"%{HTTPD_COMBINEDLOG}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z$_][a-zA-Z$_0-9]*\\.)*[a-zA-Z$_][a-zA-Z$_0-9]*"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_. -]+)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAMETHOD"=>"(?:(<(?:cl)?init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVASTACKTRACEPART"=>"%{SPACE}at %{JAVACLASS:class}\\.%{JAVAMETHOD:method}\\(%{JAVAFILE:file}(?::%{NUMBER:line})?\\)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVATHREAD"=>"(?:[A-Z]{2}-Processor[\\d]+)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z0-9-]+\\.)+[A-Za-z0-9$]+"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_.-]+)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVALOGMESSAGE"=>"(.*)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"CATALINA_DATESTAMP"=>"%{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"TOMCAT_DATESTAMP"=>"20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"CATALINALOG"=>"%{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"TOMCATLOG"=>"%{TOMCAT_DATESTAMP:timestamp} \\| %{LOGLEVEL:level} \\| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW_EVENT"=>"(RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)"}
[2020-01-30T22:27:53,721][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW1"=>"%{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \\d+\\(%{DATA:sent}\\) \\d+\\(%{DATA:received}\\) %{INT:elapsed-time} .*"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW2"=>"%{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW3"=>"%{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\\(\\d\\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424PRINTASCII"=>"[!-~]+"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGBASE2"=>"(?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGPAMSESSION"=>"%{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\\(%{DATA:pam_caller}\\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"CRON_ACTION"=>"[A-Z ]+"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"CRONLOG"=>"%{SYSLOGBASE} \\(%{USER:user}\\) %{CRON_ACTION:action} \\(%{DATA:message}\\)"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGLINE"=>"%{SYSLOGBASE2} %{GREEDYDATA:message}"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424PRI"=>"<%{NONNEGINT:syslog5424_pri}>"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424SD"=>"\\[%{DATA}\\]+"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424BASE"=>"%{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{IPORHOST:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424LINE"=>"%{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MAVEN_VERSION"=>"(?:(\\d+)\\.)?(?:(\\d+)\\.)?(\\*|\\d+)(?:[.-](RELEASE|SNAPSHOT))?"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVE"=>"., \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\]%{SPACE}%{LOGLEVEL:event_level}"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_LOG"=>"%{SYSLOGTIMESTAMP:timestamp} \\[%{WORD:component}\\] %{GREEDYDATA:message}"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_QUERY"=>"\\{ (?<={ ).*(?= } ntoreturn:) \\}"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_SLOWQUERY"=>"%{WORD} %{MONGO_WORDDASH:database}\\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_WORDDASH"=>"\\b[\\w-]+\\b"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_SEVERITY"=>"\\w"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_COMPONENT"=>"%{WORD}|-"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\\[%{DATA:context}\\])? %{GREEDYDATA:message}"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOSTIME"=>"\\[%{NUMBER:nagios_epoch}\\]"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_CURRENT_SERVICE_STATE"=>"CURRENT SERVICE STATE"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_CURRENT_HOST_STATE"=>"CURRENT HOST STATE"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_NOTIFICATION"=>"SERVICE NOTIFICATION"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_NOTIFICATION"=>"HOST NOTIFICATION"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_ALERT"=>"SERVICE ALERT"}
[2020-01-30T22:27:53,737][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_ALERT"=>"HOST ALERT"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_FLAPPING_ALERT"=>"SERVICE FLAPPING ALERT"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_FLAPPING_ALERT"=>"HOST FLAPPING ALERT"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT"=>"SERVICE DOWNTIME ALERT"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_DOWNTIME_ALERT"=>"HOST DOWNTIME ALERT"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_PASSIVE_SERVICE_CHECK"=>"PASSIVE SERVICE CHECK"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_PASSIVE_HOST_CHECK"=>"PASSIVE HOST CHECK"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_EVENT_HANDLER"=>"SERVICE EVENT HANDLER"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_EVENT_HANDLER"=>"HOST EVENT HANDLER"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_EXTERNAL_COMMAND"=>"EXTERNAL COMMAND"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_TIMEPERIOD_TRANSITION"=>"TIMEPERIOD TRANSITION"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_SVC_CHECK"=>"DISABLE_SVC_CHECK"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_SVC_CHECK"=>"ENABLE_SVC_CHECK"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_CHECK"=>"DISABLE_HOST_CHECK"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_CHECK"=>"ENABLE_HOST_CHECK"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT"=>"PROCESS_SERVICE_CHECK_RESULT"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_PROCESS_HOST_CHECK_RESULT"=>"PROCESS_HOST_CHECK_RESULT"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME"=>"SCHEDULE_SERVICE_DOWNTIME"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_SCHEDULE_HOST_DOWNTIME"=>"SCHEDULE_HOST_DOWNTIME"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS"=>"DISABLE_HOST_SVC_NOTIFICATIONS"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS"=>"ENABLE_HOST_SVC_NOTIFICATIONS"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS"=>"DISABLE_HOST_NOTIFICATIONS"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS"=>"ENABLE_HOST_NOTIFICATIONS"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS"=>"DISABLE_SVC_NOTIFICATIONS"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS"=>"ENABLE_SVC_NOTIFICATIONS"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_WARNING"=>"Warning:%{SPACE}%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_CURRENT_SERVICE_STATE"=>"%{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_CURRENT_HOST_STATE"=>"%{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_NOTIFICATION"=>"%{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_NOTIFICATION"=>"%{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_ALERT"=>"%{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_ALERT"=>"%{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:27:53,752][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_PASSIVE_SERVICE_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_PASSIVE_HOST_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_EVENT_HANDLER"=>"%{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_EVENT_HANDLER"=>"%{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TIMEPERIOD_TRANSITION"=>"%{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOSLOGLINE"=>"%{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"POSTGRESQL"=>"%{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}"}
[2020-01-30T22:27:53,768][DEBUG][logstash.filters.grok    ] Adding pattern {"RUUID"=>"\\h{32}"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"RCONTROLLER"=>"(?<controller>[^#]+)#(?<action>\\w+)"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3HEAD"=>"(?m)Started %{WORD:verb} \"%{URIPATHPARAM:request}\" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"RPROCESSING"=>"\\W*Processing by %{RCONTROLLER} as (?<format>\\S+)(?:\\W*Parameters: {%{DATA:params}}\\W*)?"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3FOOT"=>"Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3PROFILE"=>"(?:\\(Views: %{NUMBER:viewms}ms \\| ActiveRecord: %{NUMBER:activerecordms}ms|\\(ActiveRecord: %{NUMBER:activerecordms}ms)?"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3"=>"%{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\\n)*)(?:%{RAILS3FOOT})?"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISTIMESTAMP"=>"%{MONTHDAY} %{MONTH} %{TIME}"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISLOG"=>"\\[%{POSINT:pid}\\] %{REDISTIMESTAMP:timestamp} \\* "}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISMONLOG"=>"%{NUMBER:timestamp} \\[%{INT:database} %{IP:client}:%{NUMBER:port}\\] \"%{WORD:command}\"\\s?%{GREEDYDATA:params}"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"RUBY_LOGLEVEL"=>"(?:DEBUG|FATAL|ERROR|WARN|INFO)"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"RUBY_LOGGER"=>"[DFEWI], \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Adding pattern {"SQUID3"=>"%{NUMBER:timestamp}\\s+%{NUMBER:duration}\\s%{IP:client_address}\\s%{WORD:cache_result}/%{POSINT:status_code}\\s%{NUMBER:bytes}\\s%{WORD:request_method}\\s%{NOTSPACE:url}\\s(%{NOTSPACE:user}|-)\\s%{WORD:hierarchy_code}/%{IPORHOST:server}\\s%{NOTSPACE:content_type}"}
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<TIMESTAMP_ISO8601:timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?)
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?>\d\d){1,2})
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:0?[1-9]|1[0-2]))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:Z|[+-]%{HOUR}(?::?%{MINUTE})))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<LOGLEVEL:severity>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?))
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<GREEDYDATA:message>.*)
[2020-01-30T22:27:53,784][DEBUG][logstash.filters.grok    ] Grok compiled OK {:pattern=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}", :expanded_pattern=>"(?m)(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01]?[0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?) (?<LOGLEVEL:severity>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)) (?<GREEDYDATA:message>.*)"}
[2020-01-30T22:27:53,799][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
[2020-01-30T22:27:54,191][ERROR][logstash.pipeline        ] Error registering plugin {:plugin=>"<LogStash::Inputs::File path=>[\"test.log\"], start_position=>\"beginning\", codec=><LogStash::Codecs::Multiline pattern=>\"^%{TIMESTAMP_ISO8601}\", negate=>true, what=>\"previous\", id=>\"5214ce8cd6b5e57d516944cb1e2b93fb03be178c-1\", enable_metric=>true, charset=>\"UTF-8\", multiline_tag=>\"multiline\", max_lines=>500, max_bytes=>10485760>, id=>\"5214ce8cd6b5e57d516944cb1e2b93fb03be178c-2\", enable_metric=>true, stat_interval=>1, discover_interval=>15, sincedb_write_interval=>15, delimiter=>\"\\n\", close_older=>3600>", :error=>"File paths must be absolute, relative path specified: test.log"}
[2020-01-30T22:27:54,193][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x42451d00 run>"}
[2020-01-30T22:27:54,193][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0xc04b58a sleep>"}
[2020-01-30T22:27:54,194][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x4114c570 run>"}
[2020-01-30T22:27:54,194][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x23a23a92 sleep>"}
[2020-01-30T22:27:54,195][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x42451d00>
[2020-01-30T22:27:54,255][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0xc04b58a>
[2020-01-30T22:27:54,255][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x4114c570>
[2020-01-30T22:27:54,255][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x23a23a92>
[2020-01-30T22:27:54,255][DEBUG][logstash.filters.mutate  ] closing {:plugin=>"LogStash::Filters::Mutate"}
[2020-01-30T22:27:54,255][DEBUG][logstash.filters.grok    ] closing {:plugin=>"LogStash::Filters::Grok"}
[2020-01-30T22:27:54,380][DEBUG][logstash.filters.date    ] closing {:plugin=>"LogStash::Filters::Date"}
[2020-01-30T22:27:54,380][DEBUG][logstash.outputs.stdout  ] closing {:plugin=>"LogStash::Outputs::Stdout"}
[2020-01-30T22:27:54,395][ERROR][logstash.agent           ] Pipeline aborted due to error {:exception=>#<ArgumentError: File paths must be absolute, relative path specified: test.log>, :backtrace=>["D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-input-file-4.0.3/lib/logstash/inputs/file.rb:187:in `register'", "org/jruby/RubyArray.java:1613:in `each'", "D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-input-file-4.0.3/lib/logstash/inputs/file.rb:185:in `register'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:290:in `register_plugin'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:301:in `register_plugins'", "org/jruby/RubyArray.java:1613:in `each'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:301:in `register_plugins'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:456:in `start_inputs'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:348:in `start_workers'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/pipeline.rb:235:in `run'", "D:/projects/elkstack/logstash-5.6.4/logstash-core/lib/logstash/agent.rb:408:in `start_pipeline'"]}
[2020-01-30T22:27:54,411][DEBUG][logstash.agent           ] Starting puma
[2020-01-30T22:27:54,411][DEBUG][logstash.agent           ] Trying to start WebServer {:port=>9600}
[2020-01-30T22:27:54,411][DEBUG][logstash.api.service     ] [api-service] start
[2020-01-30T22:27:54,505][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-01-30T22:27:57,422][DEBUG][logstash.instrument.periodicpoller.os] PeriodicPoller: Stopping
[2020-01-30T22:27:57,422][DEBUG][logstash.instrument.periodicpoller.jvm] PeriodicPoller: Stopping
[2020-01-30T22:27:57,422][DEBUG][logstash.instrument.periodicpoller.persistentqueue] PeriodicPoller: Stopping
[2020-01-30T22:27:57,422][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] PeriodicPoller: Stopping
[2020-01-30T22:27:57,422][WARN ][logstash.agent           ] stopping pipeline {:id=>"main"}
[2020-01-30T22:27:57,422][DEBUG][logstash.pipeline        ] Closing inputs
[2020-01-30T22:27:57,422][DEBUG][logstash.inputs.file     ] stopping {:plugin=>"LogStash::Inputs::File"}
[2020-01-30T22:27:57,422][DEBUG][logstash.pipeline        ] Closed inputs
[2020-01-30T22:29:07,089][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration"}
[2020-01-30T22:29:07,089][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"fb_apache", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x7ef9ae6b @kibana_version_parts=["5", "6", "0"], @module_name="fb_apache", @directory="D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration">}
[2020-01-30T22:29:07,089][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration"}
[2020-01-30T22:29:07,089][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"netflow", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x420c68fa @kibana_version_parts=["5", "6", "0"], @module_name="netflow", @directory="D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration">}
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] -------- Logstash Settings (* means modified) ---------
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] node.name: "xxxx"
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] *path.config: "confs\\_xxxx2-lo4j.conf"
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] path.data: "D:/projects/elkstack/logstash-5.6.4/data"
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] modules.cli: []
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] modules: []
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] modules_setup: false
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] config.test_and_exit: false
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] config.reload.automatic: false
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] config.support_escapes: false
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] config.reload.interval: 3
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] metric.collect: true
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] pipeline.id: "main"
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] pipeline.system: false
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] pipeline.workers: 4
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] pipeline.output.workers: 1
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] pipeline.batch.size: 125
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] pipeline.batch.delay: 5
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] pipeline.unsafe_shutdown: false
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] path.plugins: []
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] config.debug: false
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] *log.level: "debug" (default: "info")
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] version: false
[2020-01-30T22:29:07,104][DEBUG][logstash.runner          ] help: false
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] log.format: "plain"
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] http.host: "127.0.0.1"
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] http.port: 9600..9700
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] http.environment: "production"
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] queue.type: "memory"
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] queue.drain: false
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] queue.page_capacity: 262144000
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] queue.max_bytes: 1073741824
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] queue.max_events: 0
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] queue.checkpoint.acks: 1024
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] queue.checkpoint.writes: 1024
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] queue.checkpoint.interval: 1000
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] dead_letter_queue.enable: false
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] dead_letter_queue.max_bytes: 1073741824
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] slowlog.threshold.warn: -1
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] slowlog.threshold.info: -1
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] slowlog.threshold.debug: -1
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] slowlog.threshold.trace: -1
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] path.queue: "D:/projects/elkstack/logstash-5.6.4/data/queue"
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] path.dead_letter_queue: "D:/projects/elkstack/logstash-5.6.4/data/dead_letter_queue"
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] path.settings: "D:/projects/elkstack/logstash-5.6.4/config"
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] path.logs: "D:/projects/elkstack/logstash-5.6.4/logs"
[2020-01-30T22:29:07,120][DEBUG][logstash.runner          ] --------------- Logstash Settings -------------------
[2020-01-30T22:29:07,136][DEBUG][logstash.agent           ] Agent: Configuring metric collection
[2020-01-30T22:29:07,151][DEBUG][logstash.instrument.periodicpoller.os] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:29:07,167][DEBUG][logstash.instrument.periodicpoller.jvm] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:29:07,198][DEBUG][logstash.instrument.periodicpoller.persistentqueue] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:29:07,198][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:29:07,198][DEBUG][logstash.agent           ] Reading config file {:config_file=>"D:/projects/elkstack/logstash-5.6.4/confs/_xxxx2-lo4j.conf"}
[2020-01-30T22:29:07,292][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"multiline", :type=>"codec", :class=>LogStash::Codecs::Multiline}
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@pattern = "^%{TIMESTAMP_ISO8601}"
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@negate = true
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@what = "previous"
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-1"
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@enable_metric = true
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@patterns_dir = []
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@charset = "UTF-8"
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@multiline_tag = "multiline"
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_lines = 500
[2020-01-30T22:29:07,308][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_bytes = 10485760
[2020-01-30T22:29:07,653][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/aws"}
[2020-01-30T22:29:07,653][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bacula"}
[2020-01-30T22:29:07,653][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bind"}
[2020-01-30T22:29:07,653][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bro"}
[2020-01-30T22:29:07,653][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/exim"}
[2020-01-30T22:29:07,653][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/firewalls"}
[2020-01-30T22:29:07,668][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns"}
[2020-01-30T22:29:07,668][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/haproxy"}
[2020-01-30T22:29:07,668][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/httpd"}
[2020-01-30T22:29:07,668][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/java"}
[2020-01-30T22:29:07,668][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/junos"}
[2020-01-30T22:29:07,684][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/linux-syslog"}
[2020-01-30T22:29:07,684][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/maven"}
[2020-01-30T22:29:07,684][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective"}
[2020-01-30T22:29:07,684][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective-patterns"}
[2020-01-30T22:29:07,684][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mongodb"}
[2020-01-30T22:29:07,684][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/nagios"}
[2020-01-30T22:29:07,684][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/postgresql"}
[2020-01-30T22:29:07,700][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/rails"}
[2020-01-30T22:29:07,700][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/redis"}
[2020-01-30T22:29:07,700][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/ruby"}
[2020-01-30T22:29:07,700][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/squid"}
[2020-01-30T22:29:07,747][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"file", :type=>"input", :class=>LogStash::Inputs::File}
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@pattern = "^%{TIMESTAMP_ISO8601}"
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@negate = true
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@what = "previous"
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-1"
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@enable_metric = true
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@patterns_dir = []
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@charset = "UTF-8"
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@multiline_tag = "multiline"
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_lines = 500
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_bytes = 10485760
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/aws"}
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bacula"}
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bind"}
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bro"}
[2020-01-30T22:29:07,762][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/exim"}
[2020-01-30T22:29:07,778][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/firewalls"}
[2020-01-30T22:29:07,778][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns"}
[2020-01-30T22:29:07,778][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/haproxy"}
[2020-01-30T22:29:07,778][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/httpd"}
[2020-01-30T22:29:07,778][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/java"}
[2020-01-30T22:29:07,793][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/junos"}
[2020-01-30T22:29:07,793][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/linux-syslog"}
[2020-01-30T22:29:07,793][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/maven"}
[2020-01-30T22:29:07,793][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective"}
[2020-01-30T22:29:07,793][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective-patterns"}
[2020-01-30T22:29:07,793][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mongodb"}
[2020-01-30T22:29:07,793][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/nagios"}
[2020-01-30T22:29:07,809][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/postgresql"}
[2020-01-30T22:29:07,809][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/rails"}
[2020-01-30T22:29:07,809][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/redis"}
[2020-01-30T22:29:07,809][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/ruby"}
[2020-01-30T22:29:07,809][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/squid"}
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@path = ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@start_position = "beginning"
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@codec = <LogStash::Codecs::Multiline pattern=>"^%{TIMESTAMP_ISO8601}", negate=>true, what=>"previous", id=>"8cad91d2924c871fca924a8811d1fdcd4c76e5be-1", enable_metric=>true, charset=>"UTF-8", multiline_tag=>"multiline", max_lines=>500, max_bytes=>10485760>
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-2"
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@enable_metric = true
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@add_field = {}
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@stat_interval = 1
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@discover_interval = 15
[2020-01-30T22:29:07,809][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@sincedb_write_interval = 15
[2020-01-30T22:29:07,825][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@delimiter = "\n"
[2020-01-30T22:29:07,825][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@close_older = 3600
[2020-01-30T22:29:07,825][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"mutate", :type=>"filter", :class=>LogStash::Filters::Mutate}
[2020-01-30T22:29:07,840][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@gsub = ["message", "r", ""]
[2020-01-30T22:29:07,840][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-3"
[2020-01-30T22:29:07,840][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@enable_metric = true
[2020-01-30T22:29:07,840][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@add_tag = []
[2020-01-30T22:29:07,840][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@remove_tag = []
[2020-01-30T22:29:07,840][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@add_field = {}
[2020-01-30T22:29:07,840][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@remove_field = []
[2020-01-30T22:29:07,840][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@periodic_flush = false
[2020-01-30T22:29:07,884][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"grok", :type=>"filter", :class=>LogStash::Filters::Grok}
[2020-01-30T22:29:07,888][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@match = {"message"=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}
[2020-01-30T22:29:07,889][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@overwrite = ["message"]
[2020-01-30T22:29:07,890][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-4"
[2020-01-30T22:29:07,890][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@enable_metric = true
[2020-01-30T22:29:07,890][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_tag = []
[2020-01-30T22:29:07,891][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_tag = []
[2020-01-30T22:29:07,891][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_field = {}
[2020-01-30T22:29:07,891][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_field = []
[2020-01-30T22:29:07,892][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@periodic_flush = false
[2020-01-30T22:29:07,892][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_dir = []
[2020-01-30T22:29:07,892][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@pattern_definitions = {}
[2020-01-30T22:29:07,893][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_files_glob = "*"
[2020-01-30T22:29:07,893][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@break_on_match = true
[2020-01-30T22:29:07,894][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@named_captures_only = true
[2020-01-30T22:29:07,894][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@keep_empty_captures = false
[2020-01-30T22:29:07,895][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_failure = ["_grokparsefailure"]
[2020-01-30T22:29:07,895][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_millis = 30000
[2020-01-30T22:29:07,895][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_timeout = "_groktimeout"
[2020-01-30T22:29:07,920][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"date", :type=>"filter", :class=>LogStash::Filters::Date}
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@match = ["timestamp", "yyyy-MM-dd HH:mm:ss,SSS"]
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-5"
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@enable_metric = true
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@add_tag = []
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@remove_tag = []
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@add_field = {}
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@remove_field = []
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@periodic_flush = false
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@target = "@timestamp"
[2020-01-30T22:29:07,936][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@tag_on_failure = ["_dateparsefailure"]
[2020-01-30T22:29:07,952][DEBUG][org.logstash.filters.DateFilter] Date filter with format=yyyy-MM-dd HH:mm:ss,SSS, locale=null, timezone=null built as org.logstash.filters.parser.JodaParser
[2020-01-30T22:29:07,967][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"stdout", :type=>"output", :class=>LogStash::Outputs::Stdout}
[2020-01-30T22:29:07,983][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"rubydebug", :type=>"codec", :class=>LogStash::Codecs::RubyDebug}
[2020-01-30T22:29:07,983][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@id = "rubydebug_1e43cc5b-3c13-474e-a37f-1bf6d91f22f3"
[2020-01-30T22:29:07,983][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@enable_metric = true
[2020-01-30T22:29:07,983][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@metadata = false
[2020-01-30T22:29:08,327][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@codec = <LogStash::Codecs::RubyDebug id=>"rubydebug_1e43cc5b-3c13-474e-a37f-1bf6d91f22f3", enable_metric=>true, metadata=>false>
[2020-01-30T22:29:08,327][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-6"
[2020-01-30T22:29:08,327][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@enable_metric = true
[2020-01-30T22:29:08,327][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@workers = 1
[2020-01-30T22:29:08,327][DEBUG][logstash.agent           ] starting agent
[2020-01-30T22:29:08,327][DEBUG][logstash.agent           ] starting pipeline {:id=>"main"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Grok patterns path {:paths=>["D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns", "D:/projects/elkstack/logstash-5.6.4/patterns/*"]}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Grok patterns path {:paths=>[]}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Match data {:match=>{"message"=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] regexp: /message {:pattern=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"S3_REQUEST_LINE"=>"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"S3_ACCESS_LOG"=>"%{WORD:owner} %{NOTSPACE:bucket} \\[%{HTTPDATE:timestamp}\\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:\"%{S3_REQUEST_LINE}\"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:\"?%{QS:agent}\"?|-) (?:-|%{NOTSPACE:version_id})"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_URIPATHPARAM"=>"%{URIPATH:path}(?:%{URIPARAM:params})?"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_URI"=>"%{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_REQUEST_LINE"=>"(?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_ACCESS_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} \"%{ELB_REQUEST_LINE}\""}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"CLOUDFRONT_ACCESS_LOG"=>"(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}\\t%{TIME})\\t%{WORD:x_edge_location}\\t(?:%{NUMBER:sc_bytes:int}|-)\\t%{IPORHOST:clientip}\\t%{WORD:cs_method}\\t%{HOSTNAME:cs_host}\\t%{NOTSPACE:cs_uri_stem}\\t%{NUMBER:sc_status:int}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:agent}\\t%{GREEDYDATA:cs_uri_query}\\t%{GREEDYDATA:cookies}\\t%{WORD:x_edge_result_type}\\t%{NOTSPACE:x_edge_request_id}\\t%{HOSTNAME:x_host_header}\\t%{URIPROTO:cs_protocol}\\t%{INT:cs_bytes:int}\\t%{GREEDYDATA:time_taken:float}\\t%{GREEDYDATA:x_forwarded_for}\\t%{GREEDYDATA:ssl_protocol}\\t%{GREEDYDATA:ssl_cipher}\\t%{GREEDYDATA:x_edge_response_result_type}"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_TIMESTAMP"=>"%{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_HOST"=>"[a-zA-Z0-9-]+"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_VOLUME"=>"%{USER}"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_DEVICE"=>"%{USER}"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_DEVICEPATH"=>"%{UNIXPATH}"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_CAPACITY"=>"%{INT}{1,3}(,%{INT}{3})*"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_VERSION"=>"%{USER}"}
[2020-01-30T22:29:08,342][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_JOB"=>"%{USER}"}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MAX_CAPACITY"=>"User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_END_VOLUME"=>"End of medium on Volume \\\"%{BACULA_VOLUME:volume}\\\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_VOLUME"=>"Created new Volume \\\"%{BACULA_VOLUME:volume}\\\" in catalog."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_LABEL"=>"Labeled new Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_WROTE_LABEL"=>"Wrote label to prelabeled Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_MOUNT"=>"New volume \\\"%{BACULA_VOLUME:volume}\\\" mounted on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOOPEN"=>"\\s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOOPENDIR"=>"\\s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOSTAT"=>"\\s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOJOBS"=>"There are no more Jobs associated with Volume \\\"%{BACULA_VOLUME:volume}\\\". Marking it purged."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_ALL_RECORDS_PRUNED"=>"All records pruned from Volume \\\"%{BACULA_VOLUME:volume}\\\"; marking it \\\"Purged\\\""}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_JOBS"=>"Begin pruning Jobs older than %{INT} month %{INT} days ."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_FILES"=>"Begin pruning Files."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_PRUNED_JOBS"=>"Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_PRUNED_FILES"=>"Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_ENDPRUNE"=>"End auto prune."}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_STARTJOB"=>"Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}"}
[2020-01-30T22:29:08,358][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_STARTRESTORE"=>"Start Restore Job %{BACULA_JOB:job}"}
[2020-01-30T22:29:08,373][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_USEDEVICE"=>"Using Device \\\"%{BACULA_DEVICE:device}\\\""}
[2020-01-30T22:29:08,373][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_DIFF_FS"=>"\\s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it."}
[2020-01-30T22:29:08,373][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_JOBEND"=>"Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second"}
[2020-01-30T22:29:08,373][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRUNE_JOBS"=>"No Jobs found to prune."}
[2020-01-30T22:29:08,373][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRUNE_FILES"=>"No Files found to prune."}
[2020-01-30T22:29:08,373][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_VOLUME_PREVWRITTEN"=>"Volume \\\"%{BACULA_VOLUME:volume}\\\" previously written, moving to end of data."}
[2020-01-30T22:29:08,373][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_READYAPPEND"=>"Ready to append to end of Volume \\\"%{BACULA_VOLUME:volume}\\\" size=%{INT}"}
[2020-01-30T22:29:08,382][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_CANCELLING"=>"Cancelling duplicate JobId=%{INT}."}
[2020-01-30T22:29:08,384][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MARKCANCEL"=>"JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled."}
[2020-01-30T22:29:08,386][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_CLIENT_RBJ"=>"shell command: run ClientRunBeforeJob \\\"%{GREEDYDATA:runjob}\\\""}
[2020-01-30T22:29:08,386][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_VSS"=>"(Generate )?VSS (Writer)?"}
[2020-01-30T22:29:08,387][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MAXSTART"=>"Fatal error: Job canceled because max start delay time exceeded."}
[2020-01-30T22:29:08,387][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_DUPLICATE"=>"Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed."}
[2020-01-30T22:29:08,388][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOJOBSTAT"=>"Fatal error: No Job status returned from FD."}
[2020-01-30T22:29:08,389][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_FATAL_CONN"=>"Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-01-30T22:29:08,389][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NO_CONNECT"=>"Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-01-30T22:29:08,390][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NO_AUTH"=>"Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:"}
[2020-01-30T22:29:08,390][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOSUIT"=>"No prior or suitable Full backup found in catalog. Doing FULL backup."}
[2020-01-30T22:29:08,391][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRIOR"=>"No prior Full backup Job record found."}
[2020-01-30T22:29:08,391][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_JOB"=>"(Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \\(%{BACULA_VERSION}\\):"}
[2020-01-30T22:29:08,392][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOGLINE"=>"%{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})"}
[2020-01-30T22:29:08,394][DEBUG][logstash.filters.grok    ] Adding pattern {"BIND9_TIMESTAMP"=>"%{MONTHDAY}[-]%{MONTH}[-]%{YEAR} %{TIME}"}
[2020-01-30T22:29:08,394][DEBUG][logstash.filters.grok    ] Adding pattern {"BIND9"=>"%{BIND9_TIMESTAMP:timestamp} queries: %{LOGLEVEL:loglevel}: client %{IP:clientip}#%{POSINT:clientport} \\(%{GREEDYDATA:query}\\): query: %{GREEDYDATA:query} IN %{GREEDYDATA:querytype} \\(%{IP:dns}\\)"}
[2020-01-30T22:29:08,396][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_HTTP"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{INT:trans_depth}\\t%{GREEDYDATA:method}\\t%{GREEDYDATA:domain}\\t%{GREEDYDATA:uri}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:user_agent}\\t%{NUMBER:request_body_len}\\t%{NUMBER:response_body_len}\\t%{GREEDYDATA:status_code}\\t%{GREEDYDATA:status_msg}\\t%{GREEDYDATA:info_code}\\t%{GREEDYDATA:info_msg}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:bro_tags}\\t%{GREEDYDATA:username}\\t%{GREEDYDATA:password}\\t%{GREEDYDATA:proxied}\\t%{GREEDYDATA:orig_fuids}\\t%{GREEDYDATA:orig_mime_types}\\t%{GREEDYDATA:resp_fuids}\\t%{GREEDYDATA:resp_mime_types}"}
[2020-01-30T22:29:08,399][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_DNS"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{INT:trans_id}\\t%{GREEDYDATA:query}\\t%{GREEDYDATA:qclass}\\t%{GREEDYDATA:qclass_name}\\t%{GREEDYDATA:qtype}\\t%{GREEDYDATA:qtype_name}\\t%{GREEDYDATA:rcode}\\t%{GREEDYDATA:rcode_name}\\t%{GREEDYDATA:AA}\\t%{GREEDYDATA:TC}\\t%{GREEDYDATA:RD}\\t%{GREEDYDATA:RA}\\t%{GREEDYDATA:Z}\\t%{GREEDYDATA:answers}\\t%{GREEDYDATA:TTLs}\\t%{GREEDYDATA:rejected}"}
[2020-01-30T22:29:08,400][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_CONN"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{GREEDYDATA:service}\\t%{NUMBER:duration}\\t%{NUMBER:orig_bytes}\\t%{NUMBER:resp_bytes}\\t%{GREEDYDATA:conn_state}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:missed_bytes}\\t%{GREEDYDATA:history}\\t%{GREEDYDATA:orig_pkts}\\t%{GREEDYDATA:orig_ip_bytes}\\t%{GREEDYDATA:resp_pkts}\\t%{GREEDYDATA:resp_ip_bytes}\\t%{GREEDYDATA:tunnel_parents}"}
[2020-01-30T22:29:08,402][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_FILES"=>"%{NUMBER:ts}\\t%{NOTSPACE:fuid}\\t%{IP:tx_hosts}\\t%{IP:rx_hosts}\\t%{NOTSPACE:conn_uids}\\t%{GREEDYDATA:source}\\t%{GREEDYDATA:depth}\\t%{GREEDYDATA:analyzers}\\t%{GREEDYDATA:mime_type}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:duration}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:is_orig}\\t%{GREEDYDATA:seen_bytes}\\t%{GREEDYDATA:total_bytes}\\t%{GREEDYDATA:missing_bytes}\\t%{GREEDYDATA:overflow_bytes}\\t%{GREEDYDATA:timedout}\\t%{GREEDYDATA:parent_fuid}\\t%{GREEDYDATA:md5}\\t%{GREEDYDATA:sha1}\\t%{GREEDYDATA:sha256}\\t%{GREEDYDATA:extracted}"}
[2020-01-30T22:29:08,403][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_MSGID"=>"[0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}"}
[2020-01-30T22:29:08,404][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_FLAGS"=>"(<=|[-=>*]>|[*]{2}|==)"}
[2020-01-30T22:29:08,404][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_DATE"=>"%{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_PID"=>"\\[%{POSINT}\\]"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_QT"=>"((\\d+y)?(\\d+w)?(\\d+d)?(\\d+h)?(\\d+m)?(\\d+s)?)"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_EXCLUDE_TERMS"=>"(Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_REMOTE_HOST"=>"(H=(%{NOTSPACE:remote_hostname} )?(\\(%{NOTSPACE:remote_heloname}\\) )?\\[%{IP:remote_host}\\])"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_INTERFACE"=>"(I=\\[%{IP:exim_interface}\\](:%{NUMBER:exim_interface_port}))"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_PROTOCOL"=>"(P=%{NOTSPACE:protocol})"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_MSG_SIZE"=>"(S=%{NUMBER:exim_msg_size})"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_HEADER_ID"=>"(id=%{NOTSPACE:exim_header_id})"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_SUBJECT"=>"(T=%{QS:exim_subject})"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"NETSCREENSESSIONLOG"=>"%{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_TAGGED_SYSLOG"=>"^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOTIMESTAMP"=>"%{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOTAG"=>"[A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_ACTION"=>"Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_REASON"=>"Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\\s*)*"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_DIRECTION"=>"Inbound|inbound|Outbound|outbound"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_INTERVAL"=>"first hit|%{INT}-second interval"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_XLATE_TYPE"=>"static|dynamic"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104001"=>"\\((?:Primary|Secondary)\\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104002"=>"\\((?:Primary|Secondary)\\) Switching to STANDBY - %{GREEDYDATA:switch_reason}"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104003"=>"\\((?:Primary|Secondary)\\) Switching to FAILED\\."}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104004"=>"\\((?:Primary|Secondary)\\) Switching to OK\\."}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105003"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105004"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105005"=>"\\((?:Primary|Secondary)\\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105008"=>"\\((?:Primary|Secondary)\\) Testing [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-01-30T22:29:08,405][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105009"=>"\\((?:Primary|Secondary)\\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106001"=>"%{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106006_106007_106010"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\\(%{DATA:src_fwuser}\\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\\(%{DATA:dst_fwuser}\\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106014"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\\(%{DATA:dst_fwuser}\\))? \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\)"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106015"=>"%{CISCO_ACTION:action} %{WORD:protocol} \\(%{DATA:policy_id}\\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106021"=>"%{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106023"=>"%{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\\(%{DATA:dst_fwuser}\\))?( \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\))? by access-group \"?%{DATA:policy_id}\"? \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106100_2_3"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\) -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106100"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\)(\\(%{DATA:src_fwuser}\\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\)(\\(%{DATA:src_fwuser}\\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW304001"=>"%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? Accessed URL %{IP:dst_ip}:%{GREEDYDATA:dst_url}"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW110002"=>"%{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302010"=>"%{INT:connection_count} in use, %{INT:connection_count_max} most used"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302013_302014_302015_302016"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \\(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\\))?(\\(%{DATA:src_fwuser}\\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \\(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\\))?(\\(%{DATA:dst_fwuser}\\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \\(%{DATA:user}\\))?"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302020_302021"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\\(%{DATA:fwuser}\\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \\(%{DATA:user}\\))?"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW305011"=>"%{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW313001_313004_313008"=>"%{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW313005"=>"%{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\\(%{DATA:err_src_fwuser}\\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\\(%{DATA:err_dst_fwuser}\\))? \\(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\\) on %{DATA:interface} interface\\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\\(%{DATA:orig_src_fwuser}\\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\\(%{DATA:orig_dst_fwuser}\\))?"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW321001"=>"Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW402117"=>"%{WORD:protocol}: Received a non-IPSec packet \\(protocol= %{WORD:orig_protocol}\\) from %{IP:src_ip} to %{IP:dst_ip}"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW402119"=>"%{WORD:protocol}: Received an %{WORD:orig_protocol} packet \\(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\\) from %{IP:src_ip} \\(user= %{DATA:user}\\) to %{IP:dst_ip} that failed anti-replay checking"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW419001"=>"%{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}"}
[2020-01-30T22:29:08,420][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW419002"=>"%{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW500004"=>"%{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW602303_602304"=>"%{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \\(SPI= %{DATA:spi}\\) between %{IP:src_ip} and %{IP:dst_ip} \\(user= %{DATA:user}\\) has been %{CISCO_ACTION:action}"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW710001_710002_710003_710005_710006"=>"%{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW713172"=>"Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\\s+Remote end\\s*%{DATA:is_remote_natted}\\s*behind a NAT device\\s+This\\s+end\\s*%{DATA:is_local_natted}\\s*behind a NAT device"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW733100"=>"\\[\\s*%{DATA:drop_type}\\s*\\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"SHOREWALL"=>"(%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"SFW2"=>"((%{SYSLOGTIMESTAMP})|(%{TIMESTAMP_ISO8601}))\\s*%{HOSTNAME}\\s*kernel\\S+\\s*%{NAGIOSTIME}\\s*SFW2\\-INext\\-%{NOTSPACE:nf_action}\\s*IN=%{USERNAME:nf_in_interface}.*OUT=((\\s*%{USERNAME:nf_out_interface})|(\\s*))MAC=((%{COMMONMAC:nf_dst_mac}:%{COMMONMAC:nf_src_mac})|(\\s*)).*SRC=%{IP:nf_src_ip}\\s*DST=%{IP:nf_dst_ip}.*PROTO=%{WORD:nf_protocol}((.*SPT=%{INT:nf_src_port}.*DPT=%{INT:nf_dst_port}.*)|())"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"USERNAME"=>"[a-zA-Z0-9._-]+"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"USER"=>"%{USERNAME}"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"EMAILLOCALPART"=>"[a-zA-Z][a-zA-Z0-9_.+-=:]+"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"EMAILADDRESS"=>"%{EMAILLOCALPART}@%{HOSTNAME}"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"INT"=>"(?:[+-]?(?:[0-9]+))"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE10NUM"=>"(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"NUMBER"=>"(?:%{BASE10NUM})"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE16NUM"=>"(?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE16FLOAT"=>"\\b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"POSINT"=>"\\b(?:[1-9][0-9]*)\\b"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"NONNEGINT"=>"\\b(?:[0-9]+)\\b"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"WORD"=>"\\b\\w+\\b"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"NOTSPACE"=>"\\S+"}
[2020-01-30T22:29:08,436][DEBUG][logstash.filters.grok    ] Adding pattern {"SPACE"=>"\\s*"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"DATA"=>".*?"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"GREEDYDATA"=>".*"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"QUOTEDSTRING"=>"(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"UUID"=>"[A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"URN"=>"urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"MAC"=>"(?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOMAC"=>"(?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4})"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"WINDOWSMAC"=>"(?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"COMMONMAC"=>"(?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"IPV6"=>"((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"IPV4"=>"(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"IP"=>"(?:%{IPV6}|%{IPV4})"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"HOSTNAME"=>"\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"IPORHOST"=>"(?:%{IP}|%{HOSTNAME})"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"HOSTPORT"=>"%{IPORHOST}:%{POSINT}"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"PATH"=>"(?:%{UNIXPATH}|%{WINPATH})"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"UNIXPATH"=>"(/([\\w_%!$@:.,+~-]+|\\\\.)*)+"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"TTY"=>"(?:/dev/(pts|tty([pq])?)(\\w+)?/?(?:[0-9]+))"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"WINPATH"=>"(?>[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPROTO"=>"[A-Za-z]([A-Za-z0-9+\\-.]+)+"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"URIHOST"=>"%{IPORHOST}(?::%{POSINT:port})?"}
[2020-01-30T22:29:08,452][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPATH"=>"(?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\\-]*)+"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPARAM"=>"\\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]<>]*"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPATHPARAM"=>"%{URIPATH}(?:%{URIPARAM})?"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"URI"=>"%{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTH"=>"\\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|ä)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHNUM"=>"(?:0?[1-9]|1[0-2])"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHNUM2"=>"(?:0[1-9]|1[0-2])"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHDAY"=>"(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"DAY"=>"(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"YEAR"=>"(?>\\d\\d){1,2}"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"HOUR"=>"(?:2[0123]|[01]?[0-9])"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"MINUTE"=>"(?:[0-5][0-9])"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"SECOND"=>"(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"TIME"=>"(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE_US"=>"%{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE_EU"=>"%{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"ISO8601_TIMEZONE"=>"(?:Z|[+-]%{HOUR}(?::?%{MINUTE}))"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"ISO8601_SECOND"=>"(?:%{SECOND}|60)"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"TIMESTAMP_ISO8601"=>"%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?"}
[2020-01-30T22:29:08,467][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE"=>"%{DATE_US}|%{DATE_EU}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP"=>"%{DATE}[- ]%{TIME}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"TZ"=>"(?:[APMCE][SD]T|UTC)"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_RFC822"=>"%{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_RFC2822"=>"%{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_OTHER"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_EVENTLOG"=>"%{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGTIMESTAMP"=>"%{MONTH} +%{MONTHDAY} %{TIME}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"PROG"=>"[\\x21-\\x5a\\x5c\\x5e-\\x7e]+"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGPROG"=>"%{PROG:program}(?:\\[%{POSINT:pid}\\])?"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGHOST"=>"%{IPORHOST}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGFACILITY"=>"<%{NONNEGINT:facility}.%{NONNEGINT:priority}>"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDATE"=>"%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"QS"=>"%{QUOTEDSTRING}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGBASE"=>"%{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"LOGLEVEL"=>"([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYTIME"=>"(?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYDATE"=>"%{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYCAPTUREDREQUESTHEADERS"=>"%{DATA:captured_request_headers}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYCAPTUREDRESPONSEHEADERS"=>"%{DATA:captured_response_headers}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYHTTPBASE"=>"%{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\\{%{HAPROXYCAPTUREDREQUESTHEADERS}\\})?( )?(\\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\\})?( )?\"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?\""}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYHTTP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYTCP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDUSER"=>"%{EMAILADDRESS}|%{USER}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDERROR_DATE"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"}
[2020-01-30T22:29:08,483][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_COMMONLOG"=>"%{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_COMBINEDLOG"=>"%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent}"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD20_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{LOGLEVEL:loglevel}\\] (?:\\[client %{IPORHOST:clientip}\\] ){0,1}%{GREEDYDATA:message}"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD24_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{WORD:module}:%{LOGLEVEL:loglevel}\\] \\[pid %{POSINT:pid}(:tid %{NUMBER:tid})?\\]( \\(%{POSINT:proxy_errorcode}\\)%{DATA:proxy_message}:)?( \\[client %{IPORHOST:clientip}:%{POSINT:clientport}\\])?( %{DATA:errorcode}:)? %{GREEDYDATA:message}"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_ERRORLOG"=>"%{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"COMMONAPACHELOG"=>"%{HTTPD_COMMONLOG}"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"COMBINEDAPACHELOG"=>"%{HTTPD_COMBINEDLOG}"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z$_][a-zA-Z$_0-9]*\\.)*[a-zA-Z$_][a-zA-Z$_0-9]*"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_. -]+)"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAMETHOD"=>"(?:(<(?:cl)?init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVASTACKTRACEPART"=>"%{SPACE}at %{JAVACLASS:class}\\.%{JAVAMETHOD:method}\\(%{JAVAFILE:file}(?::%{NUMBER:line})?\\)"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVATHREAD"=>"(?:[A-Z]{2}-Processor[\\d]+)"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z0-9-]+\\.)+[A-Za-z0-9$]+"}
[2020-01-30T22:29:08,499][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_.-]+)"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVALOGMESSAGE"=>"(.*)"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"CATALINA_DATESTAMP"=>"%{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"TOMCAT_DATESTAMP"=>"20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"CATALINALOG"=>"%{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"TOMCATLOG"=>"%{TOMCAT_DATESTAMP:timestamp} \\| %{LOGLEVEL:level} \\| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW_EVENT"=>"(RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW1"=>"%{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \\d+\\(%{DATA:sent}\\) \\d+\\(%{DATA:received}\\) %{INT:elapsed-time} .*"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW2"=>"%{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW3"=>"%{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\\(\\d\\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424PRINTASCII"=>"[!-~]+"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGBASE2"=>"(?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGPAMSESSION"=>"%{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\\(%{DATA:pam_caller}\\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"CRON_ACTION"=>"[A-Z ]+"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"CRONLOG"=>"%{SYSLOGBASE} \\(%{USER:user}\\) %{CRON_ACTION:action} \\(%{DATA:message}\\)"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGLINE"=>"%{SYSLOGBASE2} %{GREEDYDATA:message}"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424PRI"=>"<%{NONNEGINT:syslog5424_pri}>"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424SD"=>"\\[%{DATA}\\]+"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424BASE"=>"%{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{IPORHOST:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424LINE"=>"%{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"MAVEN_VERSION"=>"(?:(\\d+)\\.)?(?:(\\d+)\\.)?(\\*|\\d+)(?:[.-](RELEASE|SNAPSHOT))?"}
[2020-01-30T22:29:08,514][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVE"=>"., \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\]%{SPACE}%{LOGLEVEL:event_level}"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_LOG"=>"%{SYSLOGTIMESTAMP:timestamp} \\[%{WORD:component}\\] %{GREEDYDATA:message}"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_QUERY"=>"\\{ (?<={ ).*(?= } ntoreturn:) \\}"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_SLOWQUERY"=>"%{WORD} %{MONGO_WORDDASH:database}\\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_WORDDASH"=>"\\b[\\w-]+\\b"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_SEVERITY"=>"\\w"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_COMPONENT"=>"%{WORD}|-"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\\[%{DATA:context}\\])? %{GREEDYDATA:message}"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOSTIME"=>"\\[%{NUMBER:nagios_epoch}\\]"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_CURRENT_SERVICE_STATE"=>"CURRENT SERVICE STATE"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_CURRENT_HOST_STATE"=>"CURRENT HOST STATE"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_NOTIFICATION"=>"SERVICE NOTIFICATION"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_NOTIFICATION"=>"HOST NOTIFICATION"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_ALERT"=>"SERVICE ALERT"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_ALERT"=>"HOST ALERT"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_FLAPPING_ALERT"=>"SERVICE FLAPPING ALERT"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_FLAPPING_ALERT"=>"HOST FLAPPING ALERT"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT"=>"SERVICE DOWNTIME ALERT"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_DOWNTIME_ALERT"=>"HOST DOWNTIME ALERT"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_PASSIVE_SERVICE_CHECK"=>"PASSIVE SERVICE CHECK"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_PASSIVE_HOST_CHECK"=>"PASSIVE HOST CHECK"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_EVENT_HANDLER"=>"SERVICE EVENT HANDLER"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_EVENT_HANDLER"=>"HOST EVENT HANDLER"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_EXTERNAL_COMMAND"=>"EXTERNAL COMMAND"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_TIMEPERIOD_TRANSITION"=>"TIMEPERIOD TRANSITION"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_SVC_CHECK"=>"DISABLE_SVC_CHECK"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_SVC_CHECK"=>"ENABLE_SVC_CHECK"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_CHECK"=>"DISABLE_HOST_CHECK"}
[2020-01-30T22:29:08,530][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_CHECK"=>"ENABLE_HOST_CHECK"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT"=>"PROCESS_SERVICE_CHECK_RESULT"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_PROCESS_HOST_CHECK_RESULT"=>"PROCESS_HOST_CHECK_RESULT"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME"=>"SCHEDULE_SERVICE_DOWNTIME"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_SCHEDULE_HOST_DOWNTIME"=>"SCHEDULE_HOST_DOWNTIME"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS"=>"DISABLE_HOST_SVC_NOTIFICATIONS"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS"=>"ENABLE_HOST_SVC_NOTIFICATIONS"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS"=>"DISABLE_HOST_NOTIFICATIONS"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS"=>"ENABLE_HOST_NOTIFICATIONS"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS"=>"DISABLE_SVC_NOTIFICATIONS"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS"=>"ENABLE_SVC_NOTIFICATIONS"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_WARNING"=>"Warning:%{SPACE}%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_CURRENT_SERVICE_STATE"=>"%{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_CURRENT_HOST_STATE"=>"%{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_NOTIFICATION"=>"%{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_NOTIFICATION"=>"%{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_ALERT"=>"%{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_ALERT"=>"%{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:29:08,545][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_PASSIVE_SERVICE_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_PASSIVE_HOST_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_EVENT_HANDLER"=>"%{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_EVENT_HANDLER"=>"%{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TIMEPERIOD_TRANSITION"=>"%{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOSLOGLINE"=>"%{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"POSTGRESQL"=>"%{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"RUUID"=>"\\h{32}"}
[2020-01-30T22:29:08,561][DEBUG][logstash.filters.grok    ] Adding pattern {"RCONTROLLER"=>"(?<controller>[^#]+)#(?<action>\\w+)"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3HEAD"=>"(?m)Started %{WORD:verb} \"%{URIPATHPARAM:request}\" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"RPROCESSING"=>"\\W*Processing by %{RCONTROLLER} as (?<format>\\S+)(?:\\W*Parameters: {%{DATA:params}}\\W*)?"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3FOOT"=>"Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3PROFILE"=>"(?:\\(Views: %{NUMBER:viewms}ms \\| ActiveRecord: %{NUMBER:activerecordms}ms|\\(ActiveRecord: %{NUMBER:activerecordms}ms)?"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3"=>"%{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\\n)*)(?:%{RAILS3FOOT})?"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISTIMESTAMP"=>"%{MONTHDAY} %{MONTH} %{TIME}"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISLOG"=>"\\[%{POSINT:pid}\\] %{REDISTIMESTAMP:timestamp} \\* "}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISMONLOG"=>"%{NUMBER:timestamp} \\[%{INT:database} %{IP:client}:%{NUMBER:port}\\] \"%{WORD:command}\"\\s?%{GREEDYDATA:params}"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"RUBY_LOGLEVEL"=>"(?:DEBUG|FATAL|ERROR|WARN|INFO)"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"RUBY_LOGGER"=>"[DFEWI], \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Adding pattern {"SQUID3"=>"%{NUMBER:timestamp}\\s+%{NUMBER:duration}\\s%{IP:client_address}\\s%{WORD:cache_result}/%{POSINT:status_code}\\s%{NUMBER:bytes}\\s%{WORD:request_method}\\s%{NOTSPACE:url}\\s(%{NOTSPACE:user}|-)\\s%{WORD:hierarchy_code}/%{IPORHOST:server}\\s%{NOTSPACE:content_type}"}
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<TIMESTAMP_ISO8601:timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?)
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?>\d\d){1,2})
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:0?[1-9]|1[0-2]))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:Z|[+-]%{HOUR}(?::?%{MINUTE})))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<LOGLEVEL:severity>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?))
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<GREEDYDATA:message>.*)
[2020-01-30T22:29:08,577][DEBUG][logstash.filters.grok    ] Grok compiled OK {:pattern=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}", :expanded_pattern=>"(?m)(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01]?[0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?) (?<LOGLEVEL:severity>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)) (?<GREEDYDATA:message>.*)"}
[2020-01-30T22:29:08,592][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
[2020-01-30T22:29:09,017][INFO ][logstash.pipeline        ] Pipeline main started
[2020-01-30T22:29:09,033][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:29:09,033][DEBUG][logstash.inputs.file     ] _discover_file: D:/projects/elkstack/logstash-5.6.4/test.log: new: D:/projects/elkstack/logstash-5.6.4/test.log (exclude is [])
[2020-01-30T22:29:09,033][DEBUG][logstash.agent           ] Starting puma
[2020-01-30T22:29:09,033][DEBUG][logstash.agent           ] Trying to start WebServer {:port=>9600}
[2020-01-30T22:29:09,033][DEBUG][logstash.inputs.file     ] _open_file: D:/projects/elkstack/logstash-5.6.4/test.log: opening
[2020-01-30T22:29:09,048][DEBUG][logstash.api.service     ] [api-service] start
[2020-01-30T22:29:09,048][DEBUG][logstash.inputs.file     ] D:/projects/elkstack/logstash-5.6.4/test.log: initial create, no sincedb, seeking to beginning of file
[2020-01-30T22:29:09,048][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:50,778 INFO Initializing Systems Cache\r"}
[2020-01-30T22:29:09,048][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:50,778 INFO Initializing Systems Cache\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,064][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:58,666 ERROR Processing request failed\r"}
[2020-01-30T22:29:09,064][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:58,666 ERROR Processing request failed\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,095][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.\r"}
[2020-01-30T22:29:09,095][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,095][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90)\r"}
[2020-01-30T22:29:09,095][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90)\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,111][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at kafka.producer.Producer.send(Producer.scala:77)\r"}
[2020-01-30T22:29:09,111][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.095Z, "message"=>"2015-03-23 11:11:50,778 INFO Initializing Systems Cache\r"}}
[2020-01-30T22:29:09,111][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at kafka.producer.Producer.send(Producer.scala:77)\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,111][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at kafka.javaapi.producer.Producer.send(Producer.scala:42)\r"}
[2020-01-30T22:29:09,111][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at kafka.javaapi.producer.Producer.send(Producer.scala:42)\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,111][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:50,778 INFO Initializing Systems Cache 0\r"}
[2020-01-30T22:29:09,111][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:50,778 INFO Initializing Systems Cache 0\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,111][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.095Z xxxx 2015-03-23 11:11:50,778 INFO Initializing Systems Cache
}
[2020-01-30T22:29:09,111][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:51,778 INFO Initializing Systems Cache 1\r"}
[2020-01-30T22:29:09,127][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.095Z xxxx Initializing Systems Cache
}
[2020-01-30T22:29:09,127][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:51,778 INFO Initializing Systems Cache 1\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,127][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:52,778 INFO Initializing Systems Cache 2\r"}
[2020-01-30T22:29:09,127][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:52,778 INFO Initializing Systems Cache 2\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,127][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"INFO", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:50.778Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Initializing Systems Cache\r", "timestamp"=>"2015-03-23 11:11:50,778"}}
[2020-01-30T22:29:09,142][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:53,778 INFO Initializing Systems Cache 3\r"}
[2020-01-30T22:29:09,142][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:53,778 INFO Initializing Systems Cache 3\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,142][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:54,778 INFO Initializing Systems Cache 4\r"}
[2020-01-30T22:29:09,142][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:54,778 INFO Initializing Systems Cache 4\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,142][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:54,778 INFO Ex1\r"}
[2020-01-30T22:29:09,142][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:54,778 INFO Ex1\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,158][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"SomeEx\r"}
[2020-01-30T22:29:09,158][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.111Z, "message"=>"2015-03-23 11:11:58,666 ERROR Processing request failed\r\nkafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.\r\n    at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90)\r\n    at kafka.producer.Producer.send(Producer.scala:77)\r\n    at kafka.javaapi.producer.Producer.send(Producer.scala:42)\r", "tags"=>["multiline"]}}
[2020-01-30T22:29:09,158][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"SomeEx\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,158][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.127Z, "message"=>"2015-03-23 11:11:50,778 INFO Initializing Systems Cache 0\r"}}
[2020-01-30T22:29:09,158][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-01-30T22:29:09,158][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.127Z, "message"=>"2015-03-23 11:11:51,778 INFO Initializing Systems Cache 1\r"}}
[2020-01-30T22:29:09,158][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.142Z, "message"=>"2015-03-23 11:11:52,778 INFO Initializing Systems Cache 2\r"}}
[2020-01-30T22:29:09,158][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,158][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.142Z, "message"=>"2015-03-23 11:11:53,778 INFO Initializing Systems Cache 3\r"}}
[2020-01-30T22:29:09,173][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.142Z, "message"=>"2015-03-23 11:11:54,778 INFO Initializing Systems Cache 4\r"}}
[2020-01-30T22:29:09,173][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,173][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,173][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.111Z xxxx 2015-03-23 11:11:58,666 ERROR Pocessing equest failed
kafka.common.FailedToSendMessageException: Failed to send messages afte 3 ties.
    at kafka.poduce.async.DefaultEventHandle.handle(DefaultEventHandle.scala:90)
    at kafka.poduce.Poduce.send(Poduce.scala:77)
    at kafka.javaapi.poduce.Poduce.send(Poduce.scala:42)
}
[2020-01-30T22:29:09,173][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,173][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,173][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,173][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,173][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,173][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,173][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,173][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.111Z xxxx Pocessing equest failed
kafka.common.FailedToSendMessageException: Failed to send messages afte 3 ties.
    at kafka.poduce.async.DefaultEventHandle.handle(DefaultEventHandle.scala:90)
    at kafka.poduce.Poduce.send(Poduce.scala:77)
    at kafka.javaapi.poduce.Poduce.send(Poduce.scala:42)
}
[2020-01-30T22:29:09,173][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.127Z xxxx 2015-03-23 11:11:50,778 INFO Initializing Systems Cache 0
}
[2020-01-30T22:29:09,173][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.127Z xxxx Initializing Systems Cache 0
}
[2020-01-30T22:29:09,173][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.127Z xxxx 2015-03-23 11:11:51,778 INFO Initializing Systems Cache 1
}
[2020-01-30T22:29:09,189][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.127Z xxxx Initializing Systems Cache 1
}
[2020-01-30T22:29:09,189][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.142Z xxxx 2015-03-23 11:11:52,778 INFO Initializing Systems Cache 2
}
[2020-01-30T22:29:09,189][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.142Z xxxx Initializing Systems Cache 2
}
[2020-01-30T22:29:09,189][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.142Z xxxx 2015-03-23 11:11:53,778 INFO Initializing Systems Cache 3
}
[2020-01-30T22:29:09,173][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,189][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.142Z xxxx Initializing Systems Cache 3
}
[2020-01-30T22:29:09,189][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,189][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.142Z xxxx 2015-03-23 11:11:54,778 INFO Initializing Systems Cache 4
}
[2020-01-30T22:29:09,189][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,189][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,189][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-23 11:11:54,778 INFO Ex2\r"}
[2020-01-30T22:29:09,189][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-23 11:11:54,778 INFO Ex2\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,189][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"SomeEx\r"}
[2020-01-30T22:29:09,189][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.142Z xxxx Initializing Systems Cache 4
}
[2020-01-30T22:29:09,189][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"SomeEx\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,205][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:2\r"}
[2020-01-30T22:29:09,205][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"ERROR", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:58.666Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Pocessing equest failed\r\nkafka.common.FailedToSendMessageException: Failed to send messages afte 3 ties.\r\n    at kafka.poduce.async.DefaultEventHandle.handle(DefaultEventHandle.scala:90)\r\n    at kafka.poduce.Poduce.send(Poduce.scala:77)\r\n    at kafka.javaapi.poduce.Poduce.send(Poduce.scala:42)\r", "tags"=>["multiline"], "timestamp"=>"2015-03-23 11:11:58,666"}}
[2020-01-30T22:29:09,205][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:2\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,205][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:2\r"}
[2020-01-30T22:29:09,205][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:2\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,205][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,205][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"INFO", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:50.778Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Initializing Systems Cache 0\r", "timestamp"=>"2015-03-23 11:11:50,778"}}
[2020-01-30T22:29:09,205][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"INFO", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:51.778Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Initializing Systems Cache 1\r", "timestamp"=>"2015-03-23 11:11:51,778"}}
[2020-01-30T22:29:09,205][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"INFO", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:52.778Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Initializing Systems Cache 2\r", "timestamp"=>"2015-03-23 11:11:52,778"}}
[2020-01-30T22:29:09,205][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,205][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"INFO", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:53.778Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Initializing Systems Cache 3\r", "timestamp"=>"2015-03-23 11:11:53,778"}}
[2020-01-30T22:29:09,205][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,205][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,205][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"INFO", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:54.778Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Initializing Systems Cache 4\r", "timestamp"=>"2015-03-23 11:11:54,778"}}
[2020-01-30T22:29:09,205][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,205][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,205][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,205][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,205][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"    at x.y.Z:1\r"}
[2020-01-30T22:29:09,205][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"    at x.y.Z:1\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,220][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.189Z, "message"=>"2015-03-23 11:11:54,778 INFO Ex1\r\nSomeEx\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r", "tags"=>["multiline"]}}
[2020-01-30T22:29:09,220][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"2015-03-24 11:12:34,000 ERROR Y\r"}
[2020-01-30T22:29:09,220][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.189Z xxxx 2015-03-23 11:11:54,778 INFO Ex1
SomeEx
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
}
[2020-01-30T22:29:09,220][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"2015-03-24 11:12:34,000 ERROR Y\r", :match=>true, :negate=>true}
[2020-01-30T22:29:09,220][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.189Z xxxx Ex1
SomeEx
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
}
[2020-01-30T22:29:09,220][DEBUG][logstash.inputs.file     ] Received line {:path=>"D:/projects/elkstack/logstash-5.6.4/test.log", :text=>"\r"}
[2020-01-30T22:29:09,236][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"INFO", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:54.778Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Ex1\r\nSomeEx\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r", "tags"=>["multiline"], "timestamp"=>"2015-03-23 11:11:54,778"}}
[2020-01-30T22:29:09,236][DEBUG][logstash.codecs.multiline] Multiline {:pattern=>"^%{TIMESTAMP_ISO8601}", :text=>"\r", :match=>false, :negate=>true}
[2020-01-30T22:29:09,236][DEBUG][logstash.inputs.file     ] writing sincedb (delta since last write = 1580419749)
[2020-01-30T22:29:09,252][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:29:09.220Z, "message"=>"2015-03-23 11:11:54,778 INFO Ex2\r\nSomeEx\r\n    at x.y.Z:2\r\n    at x.y.Z:2\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r", "tags"=>["multiline"]}}
[2020-01-30T22:29:09,252][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:29:09.220Z xxxx 2015-03-23 11:11:54,778 INFO Ex2
SomeEx
    at x.y.Z:2
    at x.y.Z:2
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
}
[2020-01-30T22:29:09,252][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:29:09.220Z xxxx Ex2
SomeEx
    at x.y.Z:2
    at x.y.Z:2
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
    at x.y.Z:1
}
[2020-01-30T22:29:09,252][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"INFO", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-23T10:11:54.778Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Ex2\r\nSomeEx\r\n    at x.y.Z:2\r\n    at x.y.Z:2\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r\n    at x.y.Z:1\r", "tags"=>["multiline"], "timestamp"=>"2015-03-23 11:11:54,778"}}
[2020-01-30T22:29:14,017][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:19,018][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:23,286][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:29:24,024][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:29,039][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:34,043][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:38,338][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:29:39,045][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:44,047][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:49,047][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:53,390][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:29:54,051][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:29:59,052][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:04,053][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:08,475][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:30:09,055][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:14,060][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:19,074][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:23,527][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:30:24,076][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:29,080][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:34,080][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:38,580][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:30:39,083][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:44,086][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:49,086][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:53,633][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:30:54,087][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:30:59,089][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:04,092][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:08,704][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:31:09,094][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:14,096][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:19,097][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:23,785][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:31:24,097][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:29,102][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:34,103][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:38,837][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:31:39,104][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:44,106][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:49,109][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:53,892][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:31:54,111][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:31:59,111][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:32:04,115][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:32:08,960][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:32:09,116][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:32:14,119][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:32:19,123][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:32:22,664][WARN ][logstash.runner          ] SIGINT received. Shutting down the agent.
[2020-01-30T22:32:22,664][DEBUG][logstash.instrument.periodicpoller.os] PeriodicPoller: Stopping
[2020-01-30T22:32:22,664][DEBUG][logstash.instrument.periodicpoller.jvm] PeriodicPoller: Stopping
[2020-01-30T22:32:22,664][DEBUG][logstash.instrument.periodicpoller.persistentqueue] PeriodicPoller: Stopping
[2020-01-30T22:32:22,664][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] PeriodicPoller: Stopping
[2020-01-30T22:32:22,679][WARN ][logstash.agent           ] stopping pipeline {:id=>"main"}
[2020-01-30T22:32:22,679][DEBUG][logstash.pipeline        ] Closing inputs
[2020-01-30T22:32:22,679][DEBUG][logstash.inputs.file     ] stopping {:plugin=>"LogStash::Inputs::File"}
[2020-01-30T22:32:22,679][DEBUG][logstash.pipeline        ] Closed inputs
[2020-01-30T22:32:23,010][DEBUG][logstash.inputs.file     ] closing {:plugin=>"LogStash::Inputs::File"}
[2020-01-30T22:32:23,010][DEBUG][logstash.pipeline        ] Input plugins stopped! Will shutdown filter/output workers.
[2020-01-30T22:32:23,026][DEBUG][logstash.pipeline        ] filter received {"event"=>{"@version"=>"1", "host"=>"xxxx", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2020-01-30T21:32:23.010Z, "message"=>"2015-03-24 11:12:34,000 ERROR Y\r\n\r", "tags"=>["multiline"]}}
[2020-01-30T22:32:23,026][DEBUG][logstash.filters.grok    ] Running grok filter {:event=>2020-01-30T21:32:23.010Z xxxx 2015-03-24 11:12:34,000 ERROR Y

}
[2020-01-30T22:32:23,026][DEBUG][logstash.filters.grok    ] Event now:  {:event=>2020-01-30T21:32:23.010Z xxxx Y

}
[2020-01-30T22:32:23,026][DEBUG][logstash.pipeline        ] output received {"event"=>{"severity"=>"ERROR", "path"=>"D:/projects/elkstack/logstash-5.6.4/test.log", "@timestamp"=>2015-03-24T10:12:34.000Z, "@version"=>"1", "host"=>"xxxx", "message"=>"Y\r\n\r", "tags"=>["multiline"], "timestamp"=>"2015-03-24 11:12:34,000"}}
[2020-01-30T22:32:23,119][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:32:23,119][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x7f4fe221 run>"}
[2020-01-30T22:32:23,119][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x7103d10 sleep>"}
[2020-01-30T22:32:23,119][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x2c5d2435 sleep>"}
[2020-01-30T22:32:23,119][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x23b206e7 sleep>"}
[2020-01-30T22:32:23,119][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x7f4fe221>
[2020-01-30T22:32:23,229][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x7103d10>
[2020-01-30T22:32:23,229][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x2c5d2435>
[2020-01-30T22:32:23,229][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x23b206e7>
[2020-01-30T22:32:23,229][DEBUG][logstash.filters.mutate  ] closing {:plugin=>"LogStash::Filters::Mutate"}
[2020-01-30T22:32:23,229][DEBUG][logstash.filters.grok    ] closing {:plugin=>"LogStash::Filters::Grok"}
[2020-01-30T22:32:23,481][DEBUG][logstash.filters.date    ] closing {:plugin=>"LogStash::Filters::Date"}
[2020-01-30T22:32:23,481][DEBUG][logstash.outputs.stdout  ] closing {:plugin=>"LogStash::Outputs::Stdout"}
[2020-01-30T22:32:23,481][DEBUG][logstash.pipeline        ] Pipeline main has been shutdown
[2020-01-30T22:48:51,577][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration"}
[2020-01-30T22:48:51,593][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration"}
[2020-01-30T22:48:52,724][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
[2020-01-30T22:48:53,131][INFO ][logstash.pipeline        ] Pipeline main started
[2020-01-30T22:48:53,256][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-01-30T22:49:19,568][WARN ][logstash.runner          ] SIGINT received. Shutting down the agent.
[2020-01-30T22:49:19,584][WARN ][logstash.agent           ] stopping pipeline {:id=>"main"}
[2020-01-30T22:49:48,685][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration"}
[2020-01-30T22:49:48,685][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"fb_apache", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x33ad2a2a @kibana_version_parts=["5", "6", "0"], @module_name="fb_apache", @directory="D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration">}
[2020-01-30T22:49:48,685][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration"}
[2020-01-30T22:49:48,701][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"netflow", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x71b50a51 @kibana_version_parts=["5", "6", "0"], @module_name="netflow", @directory="D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration">}
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] -------- Logstash Settings (* means modified) ---------
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] node.name: "xxxx"
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] *path.config: "confs\\_xxxx2-lo4j.conf"
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] path.data: "D:/projects/elkstack/logstash-5.6.4/data"
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] modules.cli: []
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] modules: []
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] modules_setup: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] config.test_and_exit: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] config.reload.automatic: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] config.support_escapes: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] config.reload.interval: 3
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] metric.collect: true
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] pipeline.id: "main"
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] pipeline.system: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] pipeline.workers: 4
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] pipeline.output.workers: 1
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] pipeline.batch.size: 125
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] pipeline.batch.delay: 5
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] pipeline.unsafe_shutdown: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] path.plugins: []
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] config.debug: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] *log.level: "debug" (default: "info")
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] version: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] help: false
[2020-01-30T22:49:48,716][DEBUG][logstash.runner          ] log.format: "plain"
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] http.host: "127.0.0.1"
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] http.port: 9600..9700
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] http.environment: "production"
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] queue.type: "memory"
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] queue.drain: false
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] queue.page_capacity: 262144000
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] queue.max_bytes: 1073741824
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] queue.max_events: 0
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] queue.checkpoint.acks: 1024
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] queue.checkpoint.writes: 1024
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] queue.checkpoint.interval: 1000
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] dead_letter_queue.enable: false
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] dead_letter_queue.max_bytes: 1073741824
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] slowlog.threshold.warn: -1
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] slowlog.threshold.info: -1
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] slowlog.threshold.debug: -1
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] slowlog.threshold.trace: -1
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] path.queue: "D:/projects/elkstack/logstash-5.6.4/data/queue"
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] path.dead_letter_queue: "D:/projects/elkstack/logstash-5.6.4/data/dead_letter_queue"
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] path.settings: "D:/projects/elkstack/logstash-5.6.4/config"
[2020-01-30T22:49:48,732][DEBUG][logstash.runner          ] path.logs: "D:/projects/elkstack/logstash-5.6.4/logs"
[2020-01-30T22:49:48,748][DEBUG][logstash.runner          ] --------------- Logstash Settings -------------------
[2020-01-30T22:49:48,763][DEBUG][logstash.agent           ] Agent: Configuring metric collection
[2020-01-30T22:49:48,779][DEBUG][logstash.instrument.periodicpoller.os] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:49:48,795][DEBUG][logstash.instrument.periodicpoller.jvm] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:49:48,810][DEBUG][logstash.instrument.periodicpoller.persistentqueue] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:49:48,810][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] PeriodicPoller: Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-01-30T22:49:48,826][DEBUG][logstash.agent           ] Reading config file {:config_file=>"D:/projects/elkstack/logstash-5.6.4/confs/_xxxx2-lo4j.conf"}
[2020-01-30T22:49:48,935][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"multiline", :type=>"codec", :class=>LogStash::Codecs::Multiline}
[2020-01-30T22:49:48,951][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@pattern = "^%{TIMESTAMP_ISO8601}"
[2020-01-30T22:49:48,951][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@negate = true
[2020-01-30T22:49:48,951][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@what = "previous"
[2020-01-30T22:49:48,951][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-1"
[2020-01-30T22:49:48,951][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@enable_metric = true
[2020-01-30T22:49:48,951][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@patterns_dir = []
[2020-01-30T22:49:48,966][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@charset = "UTF-8"
[2020-01-30T22:49:48,966][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@multiline_tag = "multiline"
[2020-01-30T22:49:48,966][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_lines = 500
[2020-01-30T22:49:48,966][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_bytes = 10485760
[2020-01-30T22:49:49,330][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/aws"}
[2020-01-30T22:49:49,346][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bacula"}
[2020-01-30T22:49:49,346][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bind"}
[2020-01-30T22:49:49,346][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bro"}
[2020-01-30T22:49:49,346][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/exim"}
[2020-01-30T22:49:49,346][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/firewalls"}
[2020-01-30T22:49:49,362][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns"}
[2020-01-30T22:49:49,362][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/haproxy"}
[2020-01-30T22:49:49,362][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/httpd"}
[2020-01-30T22:49:49,362][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/java"}
[2020-01-30T22:49:49,377][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/junos"}
[2020-01-30T22:49:49,377][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/linux-syslog"}
[2020-01-30T22:49:49,393][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/maven"}
[2020-01-30T22:49:49,393][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective"}
[2020-01-30T22:49:49,393][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective-patterns"}
[2020-01-30T22:49:49,393][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mongodb"}
[2020-01-30T22:49:49,393][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/nagios"}
[2020-01-30T22:49:49,393][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/postgresql"}
[2020-01-30T22:49:49,393][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/rails"}
[2020-01-30T22:49:49,393][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/redis"}
[2020-01-30T22:49:49,409][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/ruby"}
[2020-01-30T22:49:49,409][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/squid"}
[2020-01-30T22:49:49,455][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"file", :type=>"input", :class=>LogStash::Inputs::File}
[2020-01-30T22:49:49,455][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@pattern = "^%{TIMESTAMP_ISO8601}"
[2020-01-30T22:49:49,455][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@negate = true
[2020-01-30T22:49:49,455][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@what = "previous"
[2020-01-30T22:49:49,455][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-1"
[2020-01-30T22:49:49,455][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@enable_metric = true
[2020-01-30T22:49:49,455][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@patterns_dir = []
[2020-01-30T22:49:49,455][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@charset = "UTF-8"
[2020-01-30T22:49:49,471][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@multiline_tag = "multiline"
[2020-01-30T22:49:49,471][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_lines = 500
[2020-01-30T22:49:49,471][DEBUG][logstash.codecs.multiline] config LogStash::Codecs::Multiline/@max_bytes = 10485760
[2020-01-30T22:49:49,471][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/aws"}
[2020-01-30T22:49:49,471][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bacula"}
[2020-01-30T22:49:49,487][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bind"}
[2020-01-30T22:49:49,487][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/bro"}
[2020-01-30T22:49:49,487][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/exim"}
[2020-01-30T22:49:49,487][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/firewalls"}
[2020-01-30T22:49:49,487][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns"}
[2020-01-30T22:49:49,487][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/haproxy"}
[2020-01-30T22:49:49,502][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/httpd"}
[2020-01-30T22:49:49,502][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/java"}
[2020-01-30T22:49:49,502][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/junos"}
[2020-01-30T22:49:49,502][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/linux-syslog"}
[2020-01-30T22:49:49,502][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/maven"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mcollective-patterns"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/mongodb"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/nagios"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/postgresql"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/rails"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/redis"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/ruby"}
[2020-01-30T22:49:49,518][DEBUG][logstash.codecs.multiline] Grok loading patterns from file {:path=>"D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/squid"}
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@path = ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@start_position = "beginning"
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@codec = <LogStash::Codecs::Multiline pattern=>"^%{TIMESTAMP_ISO8601}", negate=>true, what=>"previous", id=>"8cad91d2924c871fca924a8811d1fdcd4c76e5be-1", enable_metric=>true, charset=>"UTF-8", multiline_tag=>"multiline", max_lines=>500, max_bytes=>10485760>
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-2"
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@enable_metric = true
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@add_field = {}
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@stat_interval = 1
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@discover_interval = 15
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@sincedb_write_interval = 15
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@delimiter = "\n"
[2020-01-30T22:49:49,534][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@close_older = 3600
[2020-01-30T22:49:49,549][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"mutate", :type=>"filter", :class=>LogStash::Filters::Mutate}
[2020-01-30T22:49:49,549][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@gsub = ["message", "r", ""]
[2020-01-30T22:49:49,549][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-3"
[2020-01-30T22:49:49,549][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@enable_metric = true
[2020-01-30T22:49:49,549][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@add_tag = []
[2020-01-30T22:49:49,549][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@remove_tag = []
[2020-01-30T22:49:49,549][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@add_field = {}
[2020-01-30T22:49:49,565][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@remove_field = []
[2020-01-30T22:49:49,565][DEBUG][logstash.filters.mutate  ] config LogStash::Filters::Mutate/@periodic_flush = false
[2020-01-30T22:49:49,580][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"grok", :type=>"filter", :class=>LogStash::Filters::Grok}
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@match = {"message"=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@overwrite = ["message"]
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-4"
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@enable_metric = true
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_tag = []
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_tag = []
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_field = {}
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_field = []
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@periodic_flush = false
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_dir = []
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@pattern_definitions = {}
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_files_glob = "*"
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@break_on_match = true
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@named_captures_only = true
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@keep_empty_captures = false
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_failure = ["_grokparsefailure"]
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_millis = 30000
[2020-01-30T22:49:49,596][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_timeout = "_groktimeout"
[2020-01-30T22:49:49,627][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"date", :type=>"filter", :class=>LogStash::Filters::Date}
[2020-01-30T22:49:49,643][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@match = ["timestamp", "yyyy-MM-dd HH:mm:ss,SSS"]
[2020-01-30T22:49:49,646][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-5"
[2020-01-30T22:49:49,647][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@enable_metric = true
[2020-01-30T22:49:49,647][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@add_tag = []
[2020-01-30T22:49:49,647][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@remove_tag = []
[2020-01-30T22:49:49,649][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@add_field = {}
[2020-01-30T22:49:49,649][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@remove_field = []
[2020-01-30T22:49:49,650][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@periodic_flush = false
[2020-01-30T22:49:49,651][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@target = "@timestamp"
[2020-01-30T22:49:49,651][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@tag_on_failure = ["_dateparsefailure"]
[2020-01-30T22:49:49,665][DEBUG][org.logstash.filters.DateFilter] Date filter with format=yyyy-MM-dd HH:mm:ss,SSS, locale=null, timezone=null built as org.logstash.filters.parser.JodaParser
[2020-01-30T22:49:49,676][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"stdout", :type=>"output", :class=>LogStash::Outputs::Stdout}
[2020-01-30T22:49:49,677][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"rubydebug", :type=>"codec", :class=>LogStash::Codecs::RubyDebug}
[2020-01-30T22:49:49,677][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@id = "rubydebug_b672f353-e80e-46e3-b146-0cd0ae3d3724"
[2020-01-30T22:49:49,677][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@enable_metric = true
[2020-01-30T22:49:49,677][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@metadata = false
[2020-01-30T22:49:50,036][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@codec = <LogStash::Codecs::RubyDebug id=>"rubydebug_b672f353-e80e-46e3-b146-0cd0ae3d3724", enable_metric=>true, metadata=>false>
[2020-01-30T22:49:50,036][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@id = "8cad91d2924c871fca924a8811d1fdcd4c76e5be-6"
[2020-01-30T22:49:50,036][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@enable_metric = true
[2020-01-30T22:49:50,036][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@workers = 1
[2020-01-30T22:49:50,052][DEBUG][logstash.agent           ] starting agent
[2020-01-30T22:49:50,052][DEBUG][logstash.agent           ] starting pipeline {:id=>"main"}
[2020-01-30T22:49:50,052][DEBUG][logstash.filters.grok    ] Grok patterns path {:paths=>["D:/projects/elkstack/logstash-5.6.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns", "D:/projects/elkstack/logstash-5.6.4/patterns/*"]}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Grok patterns path {:paths=>[]}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Match data {:match=>{"message"=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] regexp: /message {:pattern=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}"}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Adding pattern {"S3_REQUEST_LINE"=>"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Adding pattern {"S3_ACCESS_LOG"=>"%{WORD:owner} %{NOTSPACE:bucket} \\[%{HTTPDATE:timestamp}\\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:\"%{S3_REQUEST_LINE}\"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:\"?%{QS:agent}\"?|-) (?:-|%{NOTSPACE:version_id})"}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_URIPATHPARAM"=>"%{URIPATH:path}(?:%{URIPARAM:params})?"}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_URI"=>"%{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?"}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_REQUEST_LINE"=>"(?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Adding pattern {"ELB_ACCESS_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} \"%{ELB_REQUEST_LINE}\""}
[2020-01-30T22:49:50,067][DEBUG][logstash.filters.grok    ] Adding pattern {"CLOUDFRONT_ACCESS_LOG"=>"(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}\\t%{TIME})\\t%{WORD:x_edge_location}\\t(?:%{NUMBER:sc_bytes:int}|-)\\t%{IPORHOST:clientip}\\t%{WORD:cs_method}\\t%{HOSTNAME:cs_host}\\t%{NOTSPACE:cs_uri_stem}\\t%{NUMBER:sc_status:int}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:agent}\\t%{GREEDYDATA:cs_uri_query}\\t%{GREEDYDATA:cookies}\\t%{WORD:x_edge_result_type}\\t%{NOTSPACE:x_edge_request_id}\\t%{HOSTNAME:x_host_header}\\t%{URIPROTO:cs_protocol}\\t%{INT:cs_bytes:int}\\t%{GREEDYDATA:time_taken:float}\\t%{GREEDYDATA:x_forwarded_for}\\t%{GREEDYDATA:ssl_protocol}\\t%{GREEDYDATA:ssl_cipher}\\t%{GREEDYDATA:x_edge_response_result_type}"}
[2020-01-30T22:49:50,083][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_TIMESTAMP"=>"%{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}"}
[2020-01-30T22:49:50,083][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_HOST"=>"[a-zA-Z0-9-]+"}
[2020-01-30T22:49:50,083][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_VOLUME"=>"%{USER}"}
[2020-01-30T22:49:50,083][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_DEVICE"=>"%{USER}"}
[2020-01-30T22:49:50,083][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_DEVICEPATH"=>"%{UNIXPATH}"}
[2020-01-30T22:49:50,083][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_CAPACITY"=>"%{INT}{1,3}(,%{INT}{3})*"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_VERSION"=>"%{USER}"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_JOB"=>"%{USER}"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MAX_CAPACITY"=>"User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_END_VOLUME"=>"End of medium on Volume \\\"%{BACULA_VOLUME:volume}\\\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_VOLUME"=>"Created new Volume \\\"%{BACULA_VOLUME:volume}\\\" in catalog."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_LABEL"=>"Labeled new Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_WROTE_LABEL"=>"Wrote label to prelabeled Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NEW_MOUNT"=>"New volume \\\"%{BACULA_VOLUME:volume}\\\" mounted on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOOPEN"=>"\\s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOOPENDIR"=>"\\s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOSTAT"=>"\\s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOJOBS"=>"There are no more Jobs associated with Volume \\\"%{BACULA_VOLUME:volume}\\\". Marking it purged."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_ALL_RECORDS_PRUNED"=>"All records pruned from Volume \\\"%{BACULA_VOLUME:volume}\\\"; marking it \\\"Purged\\\""}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_JOBS"=>"Begin pruning Jobs older than %{INT} month %{INT} days ."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_FILES"=>"Begin pruning Files."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_PRUNED_JOBS"=>"Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_PRUNED_FILES"=>"Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_ENDPRUNE"=>"End auto prune."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_STARTJOB"=>"Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_STARTRESTORE"=>"Start Restore Job %{BACULA_JOB:job}"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_USEDEVICE"=>"Using Device \\\"%{BACULA_DEVICE:device}\\\""}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_DIFF_FS"=>"\\s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_JOBEND"=>"Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRUNE_JOBS"=>"No Jobs found to prune."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRUNE_FILES"=>"No Files found to prune."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_VOLUME_PREVWRITTEN"=>"Volume \\\"%{BACULA_VOLUME:volume}\\\" previously written, moving to end of data."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_READYAPPEND"=>"Ready to append to end of Volume \\\"%{BACULA_VOLUME:volume}\\\" size=%{INT}"}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_CANCELLING"=>"Cancelling duplicate JobId=%{INT}."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MARKCANCEL"=>"JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled."}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_CLIENT_RBJ"=>"shell command: run ClientRunBeforeJob \\\"%{GREEDYDATA:runjob}\\\""}
[2020-01-30T22:49:50,098][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_VSS"=>"(Generate )?VSS (Writer)?"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_MAXSTART"=>"Fatal error: Job canceled because max start delay time exceeded."}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_DUPLICATE"=>"Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed."}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOJOBSTAT"=>"Fatal error: No Job status returned from FD."}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_FATAL_CONN"=>"Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NO_CONNECT"=>"Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NO_AUTH"=>"Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOSUIT"=>"No prior or suitable Full backup found in catalog. Doing FULL backup."}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_NOPRIOR"=>"No prior Full backup Job record found."}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOG_JOB"=>"(Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \\(%{BACULA_VERSION}\\):"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BACULA_LOGLINE"=>"%{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BIND9_TIMESTAMP"=>"%{MONTHDAY}[-]%{MONTH}[-]%{YEAR} %{TIME}"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BIND9"=>"%{BIND9_TIMESTAMP:timestamp} queries: %{LOGLEVEL:loglevel}: client %{IP:clientip}#%{POSINT:clientport} \\(%{GREEDYDATA:query}\\): query: %{GREEDYDATA:query} IN %{GREEDYDATA:querytype} \\(%{IP:dns}\\)"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_HTTP"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{INT:trans_depth}\\t%{GREEDYDATA:method}\\t%{GREEDYDATA:domain}\\t%{GREEDYDATA:uri}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:user_agent}\\t%{NUMBER:request_body_len}\\t%{NUMBER:response_body_len}\\t%{GREEDYDATA:status_code}\\t%{GREEDYDATA:status_msg}\\t%{GREEDYDATA:info_code}\\t%{GREEDYDATA:info_msg}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:bro_tags}\\t%{GREEDYDATA:username}\\t%{GREEDYDATA:password}\\t%{GREEDYDATA:proxied}\\t%{GREEDYDATA:orig_fuids}\\t%{GREEDYDATA:orig_mime_types}\\t%{GREEDYDATA:resp_fuids}\\t%{GREEDYDATA:resp_mime_types}"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_DNS"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{INT:trans_id}\\t%{GREEDYDATA:query}\\t%{GREEDYDATA:qclass}\\t%{GREEDYDATA:qclass_name}\\t%{GREEDYDATA:qtype}\\t%{GREEDYDATA:qtype_name}\\t%{GREEDYDATA:rcode}\\t%{GREEDYDATA:rcode_name}\\t%{GREEDYDATA:AA}\\t%{GREEDYDATA:TC}\\t%{GREEDYDATA:RD}\\t%{GREEDYDATA:RA}\\t%{GREEDYDATA:Z}\\t%{GREEDYDATA:answers}\\t%{GREEDYDATA:TTLs}\\t%{GREEDYDATA:rejected}"}
[2020-01-30T22:49:50,114][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_CONN"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{GREEDYDATA:service}\\t%{NUMBER:duration}\\t%{NUMBER:orig_bytes}\\t%{NUMBER:resp_bytes}\\t%{GREEDYDATA:conn_state}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:missed_bytes}\\t%{GREEDYDATA:history}\\t%{GREEDYDATA:orig_pkts}\\t%{GREEDYDATA:orig_ip_bytes}\\t%{GREEDYDATA:resp_pkts}\\t%{GREEDYDATA:resp_ip_bytes}\\t%{GREEDYDATA:tunnel_parents}"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"BRO_FILES"=>"%{NUMBER:ts}\\t%{NOTSPACE:fuid}\\t%{IP:tx_hosts}\\t%{IP:rx_hosts}\\t%{NOTSPACE:conn_uids}\\t%{GREEDYDATA:source}\\t%{GREEDYDATA:depth}\\t%{GREEDYDATA:analyzers}\\t%{GREEDYDATA:mime_type}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:duration}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:is_orig}\\t%{GREEDYDATA:seen_bytes}\\t%{GREEDYDATA:total_bytes}\\t%{GREEDYDATA:missing_bytes}\\t%{GREEDYDATA:overflow_bytes}\\t%{GREEDYDATA:timedout}\\t%{GREEDYDATA:parent_fuid}\\t%{GREEDYDATA:md5}\\t%{GREEDYDATA:sha1}\\t%{GREEDYDATA:sha256}\\t%{GREEDYDATA:extracted}"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_MSGID"=>"[0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_FLAGS"=>"(<=|[-=>*]>|[*]{2}|==)"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_DATE"=>"%{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_PID"=>"\\[%{POSINT}\\]"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_QT"=>"((\\d+y)?(\\d+w)?(\\d+d)?(\\d+h)?(\\d+m)?(\\d+s)?)"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_EXCLUDE_TERMS"=>"(Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_REMOTE_HOST"=>"(H=(%{NOTSPACE:remote_hostname} )?(\\(%{NOTSPACE:remote_heloname}\\) )?\\[%{IP:remote_host}\\])"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_INTERFACE"=>"(I=\\[%{IP:exim_interface}\\](:%{NUMBER:exim_interface_port}))"}
[2020-01-30T22:49:50,130][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_PROTOCOL"=>"(P=%{NOTSPACE:protocol})"}
[2020-01-30T22:49:50,145][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_MSG_SIZE"=>"(S=%{NUMBER:exim_msg_size})"}
[2020-01-30T22:49:50,145][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_HEADER_ID"=>"(id=%{NOTSPACE:exim_header_id})"}
[2020-01-30T22:49:50,145][DEBUG][logstash.filters.grok    ] Adding pattern {"EXIM_SUBJECT"=>"(T=%{QS:exim_subject})"}
[2020-01-30T22:49:50,145][DEBUG][logstash.filters.grok    ] Adding pattern {"NETSCREENSESSIONLOG"=>"%{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_TAGGED_SYSLOG"=>"^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOTIMESTAMP"=>"%{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOTAG"=>"[A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_ACTION"=>"Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_REASON"=>"Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\\s*)*"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_DIRECTION"=>"Inbound|inbound|Outbound|outbound"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_INTERVAL"=>"first hit|%{INT}-second interval"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCO_XLATE_TYPE"=>"static|dynamic"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104001"=>"\\((?:Primary|Secondary)\\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104002"=>"\\((?:Primary|Secondary)\\) Switching to STANDBY - %{GREEDYDATA:switch_reason}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104003"=>"\\((?:Primary|Secondary)\\) Switching to FAILED\\."}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW104004"=>"\\((?:Primary|Secondary)\\) Switching to OK\\."}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105003"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105004"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105005"=>"\\((?:Primary|Secondary)\\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105008"=>"\\((?:Primary|Secondary)\\) Testing [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW105009"=>"\\((?:Primary|Secondary)\\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106001"=>"%{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106006_106007_106010"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\\(%{DATA:src_fwuser}\\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\\(%{DATA:dst_fwuser}\\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106014"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\\(%{DATA:dst_fwuser}\\))? \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\)"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106015"=>"%{CISCO_ACTION:action} %{WORD:protocol} \\(%{DATA:policy_id}\\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106021"=>"%{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106023"=>"%{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\\(%{DATA:dst_fwuser}\\))?( \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\))? by access-group \"?%{DATA:policy_id}\"? \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:49:50,161][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106100_2_3"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\) -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:49:50,177][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW106100"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\)(\\(%{DATA:src_fwuser}\\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\)(\\(%{DATA:src_fwuser}\\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-01-30T22:49:50,179][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW304001"=>"%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? Accessed URL %{IP:dst_ip}:%{GREEDYDATA:dst_url}"}
[2020-01-30T22:49:50,180][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW110002"=>"%{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:49:50,180][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302010"=>"%{INT:connection_count} in use, %{INT:connection_count_max} most used"}
[2020-01-30T22:49:50,181][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302013_302014_302015_302016"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \\(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\\))?(\\(%{DATA:src_fwuser}\\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \\(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\\))?(\\(%{DATA:dst_fwuser}\\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \\(%{DATA:user}\\))?"}
[2020-01-30T22:49:50,183][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW302020_302021"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\\(%{DATA:fwuser}\\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \\(%{DATA:user}\\))?"}
[2020-01-30T22:49:50,185][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW305011"=>"%{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}"}
[2020-01-30T22:49:50,186][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW313001_313004_313008"=>"%{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?"}
[2020-01-30T22:49:50,186][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW313005"=>"%{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\\(%{DATA:err_src_fwuser}\\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\\(%{DATA:err_dst_fwuser}\\))? \\(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\\) on %{DATA:interface} interface\\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\\(%{DATA:orig_src_fwuser}\\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\\(%{DATA:orig_dst_fwuser}\\))?"}
[2020-01-30T22:49:50,187][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW321001"=>"Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system"}
[2020-01-30T22:49:50,188][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW402117"=>"%{WORD:protocol}: Received a non-IPSec packet \\(protocol= %{WORD:orig_protocol}\\) from %{IP:src_ip} to %{IP:dst_ip}"}
[2020-01-30T22:49:50,188][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW402119"=>"%{WORD:protocol}: Received an %{WORD:orig_protocol} packet \\(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\\) from %{IP:src_ip} \\(user= %{DATA:user}\\) to %{IP:dst_ip} that failed anti-replay checking"}
[2020-01-30T22:49:50,189][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW419001"=>"%{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}"}
[2020-01-30T22:49:50,190][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW419002"=>"%{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number"}
[2020-01-30T22:49:50,190][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW500004"=>"%{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:49:50,191][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW602303_602304"=>"%{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \\(SPI= %{DATA:spi}\\) between %{IP:src_ip} and %{IP:dst_ip} \\(user= %{DATA:user}\\) has been %{CISCO_ACTION:action}"}
[2020-01-30T22:49:50,191][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW710001_710002_710003_710005_710006"=>"%{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}"}
[2020-01-30T22:49:50,192][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW713172"=>"Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\\s+Remote end\\s*%{DATA:is_remote_natted}\\s*behind a NAT device\\s+This\\s+end\\s*%{DATA:is_local_natted}\\s*behind a NAT device"}
[2020-01-30T22:49:50,192][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOFW733100"=>"\\[\\s*%{DATA:drop_type}\\s*\\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}"}
[2020-01-30T22:49:50,193][DEBUG][logstash.filters.grok    ] Adding pattern {"SHOREWALL"=>"(%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)"}
[2020-01-30T22:49:50,194][DEBUG][logstash.filters.grok    ] Adding pattern {"SFW2"=>"((%{SYSLOGTIMESTAMP})|(%{TIMESTAMP_ISO8601}))\\s*%{HOSTNAME}\\s*kernel\\S+\\s*%{NAGIOSTIME}\\s*SFW2\\-INext\\-%{NOTSPACE:nf_action}\\s*IN=%{USERNAME:nf_in_interface}.*OUT=((\\s*%{USERNAME:nf_out_interface})|(\\s*))MAC=((%{COMMONMAC:nf_dst_mac}:%{COMMONMAC:nf_src_mac})|(\\s*)).*SRC=%{IP:nf_src_ip}\\s*DST=%{IP:nf_dst_ip}.*PROTO=%{WORD:nf_protocol}((.*SPT=%{INT:nf_src_port}.*DPT=%{INT:nf_dst_port}.*)|())"}
[2020-01-30T22:49:50,197][DEBUG][logstash.filters.grok    ] Adding pattern {"USERNAME"=>"[a-zA-Z0-9._-]+"}
[2020-01-30T22:49:50,197][DEBUG][logstash.filters.grok    ] Adding pattern {"USER"=>"%{USERNAME}"}
[2020-01-30T22:49:50,197][DEBUG][logstash.filters.grok    ] Adding pattern {"EMAILLOCALPART"=>"[a-zA-Z][a-zA-Z0-9_.+-=:]+"}
[2020-01-30T22:49:50,198][DEBUG][logstash.filters.grok    ] Adding pattern {"EMAILADDRESS"=>"%{EMAILLOCALPART}@%{HOSTNAME}"}
[2020-01-30T22:49:50,198][DEBUG][logstash.filters.grok    ] Adding pattern {"INT"=>"(?:[+-]?(?:[0-9]+))"}
[2020-01-30T22:49:50,199][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE10NUM"=>"(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))"}
[2020-01-30T22:49:50,200][DEBUG][logstash.filters.grok    ] Adding pattern {"NUMBER"=>"(?:%{BASE10NUM})"}
[2020-01-30T22:49:50,200][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE16NUM"=>"(?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))"}
[2020-01-30T22:49:50,200][DEBUG][logstash.filters.grok    ] Adding pattern {"BASE16FLOAT"=>"\\b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b"}
[2020-01-30T22:49:50,201][DEBUG][logstash.filters.grok    ] Adding pattern {"POSINT"=>"\\b(?:[1-9][0-9]*)\\b"}
[2020-01-30T22:49:50,201][DEBUG][logstash.filters.grok    ] Adding pattern {"NONNEGINT"=>"\\b(?:[0-9]+)\\b"}
[2020-01-30T22:49:50,202][DEBUG][logstash.filters.grok    ] Adding pattern {"WORD"=>"\\b\\w+\\b"}
[2020-01-30T22:49:50,202][DEBUG][logstash.filters.grok    ] Adding pattern {"NOTSPACE"=>"\\S+"}
[2020-01-30T22:49:50,202][DEBUG][logstash.filters.grok    ] Adding pattern {"SPACE"=>"\\s*"}
[2020-01-30T22:49:50,203][DEBUG][logstash.filters.grok    ] Adding pattern {"DATA"=>".*?"}
[2020-01-30T22:49:50,203][DEBUG][logstash.filters.grok    ] Adding pattern {"GREEDYDATA"=>".*"}
[2020-01-30T22:49:50,204][DEBUG][logstash.filters.grok    ] Adding pattern {"QUOTEDSTRING"=>"(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))"}
[2020-01-30T22:49:50,204][DEBUG][logstash.filters.grok    ] Adding pattern {"UUID"=>"[A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}"}
[2020-01-30T22:49:50,205][DEBUG][logstash.filters.grok    ] Adding pattern {"URN"=>"urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+"}
[2020-01-30T22:49:50,205][DEBUG][logstash.filters.grok    ] Adding pattern {"MAC"=>"(?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})"}
[2020-01-30T22:49:50,206][DEBUG][logstash.filters.grok    ] Adding pattern {"CISCOMAC"=>"(?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4})"}
[2020-01-30T22:49:50,206][DEBUG][logstash.filters.grok    ] Adding pattern {"WINDOWSMAC"=>"(?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})"}
[2020-01-30T22:49:50,206][DEBUG][logstash.filters.grok    ] Adding pattern {"COMMONMAC"=>"(?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})"}
[2020-01-30T22:49:50,207][DEBUG][logstash.filters.grok    ] Adding pattern {"IPV6"=>"((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?"}
[2020-01-30T22:49:50,209][DEBUG][logstash.filters.grok    ] Adding pattern {"IPV4"=>"(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])"}
[2020-01-30T22:49:50,209][DEBUG][logstash.filters.grok    ] Adding pattern {"IP"=>"(?:%{IPV6}|%{IPV4})"}
[2020-01-30T22:49:50,209][DEBUG][logstash.filters.grok    ] Adding pattern {"HOSTNAME"=>"\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)"}
[2020-01-30T22:49:50,210][DEBUG][logstash.filters.grok    ] Adding pattern {"IPORHOST"=>"(?:%{IP}|%{HOSTNAME})"}
[2020-01-30T22:49:50,210][DEBUG][logstash.filters.grok    ] Adding pattern {"HOSTPORT"=>"%{IPORHOST}:%{POSINT}"}
[2020-01-30T22:49:50,211][DEBUG][logstash.filters.grok    ] Adding pattern {"PATH"=>"(?:%{UNIXPATH}|%{WINPATH})"}
[2020-01-30T22:49:50,211][DEBUG][logstash.filters.grok    ] Adding pattern {"UNIXPATH"=>"(/([\\w_%!$@:.,+~-]+|\\\\.)*)+"}
[2020-01-30T22:49:50,211][DEBUG][logstash.filters.grok    ] Adding pattern {"TTY"=>"(?:/dev/(pts|tty([pq])?)(\\w+)?/?(?:[0-9]+))"}
[2020-01-30T22:49:50,212][DEBUG][logstash.filters.grok    ] Adding pattern {"WINPATH"=>"(?>[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+"}
[2020-01-30T22:49:50,212][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPROTO"=>"[A-Za-z]([A-Za-z0-9+\\-.]+)+"}
[2020-01-30T22:49:50,213][DEBUG][logstash.filters.grok    ] Adding pattern {"URIHOST"=>"%{IPORHOST}(?::%{POSINT:port})?"}
[2020-01-30T22:49:50,213][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPATH"=>"(?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\\-]*)+"}
[2020-01-30T22:49:50,214][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPARAM"=>"\\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]<>]*"}
[2020-01-30T22:49:50,214][DEBUG][logstash.filters.grok    ] Adding pattern {"URIPATHPARAM"=>"%{URIPATH}(?:%{URIPARAM})?"}
[2020-01-30T22:49:50,215][DEBUG][logstash.filters.grok    ] Adding pattern {"URI"=>"%{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?"}
[2020-01-30T22:49:50,215][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTH"=>"\\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|ä)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b"}
[2020-01-30T22:49:50,216][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHNUM"=>"(?:0?[1-9]|1[0-2])"}
[2020-01-30T22:49:50,217][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHNUM2"=>"(?:0[1-9]|1[0-2])"}
[2020-01-30T22:49:50,217][DEBUG][logstash.filters.grok    ] Adding pattern {"MONTHDAY"=>"(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])"}
[2020-01-30T22:49:50,217][DEBUG][logstash.filters.grok    ] Adding pattern {"DAY"=>"(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)"}
[2020-01-30T22:49:50,218][DEBUG][logstash.filters.grok    ] Adding pattern {"YEAR"=>"(?>\\d\\d){1,2}"}
[2020-01-30T22:49:50,218][DEBUG][logstash.filters.grok    ] Adding pattern {"HOUR"=>"(?:2[0123]|[01]?[0-9])"}
[2020-01-30T22:49:50,219][DEBUG][logstash.filters.grok    ] Adding pattern {"MINUTE"=>"(?:[0-5][0-9])"}
[2020-01-30T22:49:50,219][DEBUG][logstash.filters.grok    ] Adding pattern {"SECOND"=>"(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)"}
[2020-01-30T22:49:50,219][DEBUG][logstash.filters.grok    ] Adding pattern {"TIME"=>"(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])"}
[2020-01-30T22:49:50,220][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE_US"=>"%{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}"}
[2020-01-30T22:49:50,220][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE_EU"=>"%{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}"}
[2020-01-30T22:49:50,221][DEBUG][logstash.filters.grok    ] Adding pattern {"ISO8601_TIMEZONE"=>"(?:Z|[+-]%{HOUR}(?::?%{MINUTE}))"}
[2020-01-30T22:49:50,221][DEBUG][logstash.filters.grok    ] Adding pattern {"ISO8601_SECOND"=>"(?:%{SECOND}|60)"}
[2020-01-30T22:49:50,221][DEBUG][logstash.filters.grok    ] Adding pattern {"TIMESTAMP_ISO8601"=>"%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?"}
[2020-01-30T22:49:50,222][DEBUG][logstash.filters.grok    ] Adding pattern {"DATE"=>"%{DATE_US}|%{DATE_EU}"}
[2020-01-30T22:49:50,222][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP"=>"%{DATE}[- ]%{TIME}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"TZ"=>"(?:[APMCE][SD]T|UTC)"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_RFC822"=>"%{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_RFC2822"=>"%{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_OTHER"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"DATESTAMP_EVENTLOG"=>"%{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGTIMESTAMP"=>"%{MONTH} +%{MONTHDAY} %{TIME}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"PROG"=>"[\\x21-\\x5a\\x5c\\x5e-\\x7e]+"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGPROG"=>"%{PROG:program}(?:\\[%{POSINT:pid}\\])?"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGHOST"=>"%{IPORHOST}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGFACILITY"=>"<%{NONNEGINT:facility}.%{NONNEGINT:priority}>"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDATE"=>"%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"QS"=>"%{QUOTEDSTRING}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGBASE"=>"%{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"LOGLEVEL"=>"([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYTIME"=>"(?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYDATE"=>"%{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYCAPTUREDREQUESTHEADERS"=>"%{DATA:captured_request_headers}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYCAPTUREDRESPONSEHEADERS"=>"%{DATA:captured_response_headers}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYHTTPBASE"=>"%{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\\{%{HAPROXYCAPTUREDREQUESTHEADERS}\\})?( )?(\\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\\})?( )?\"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?\""}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYHTTP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HAPROXYTCP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDUSER"=>"%{EMAILADDRESS}|%{USER}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPDERROR_DATE"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_COMMONLOG"=>"%{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_COMBINEDLOG"=>"%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent}"}
[2020-01-30T22:49:50,223][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD20_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{LOGLEVEL:loglevel}\\] (?:\\[client %{IPORHOST:clientip}\\] ){0,1}%{GREEDYDATA:message}"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD24_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{WORD:module}:%{LOGLEVEL:loglevel}\\] \\[pid %{POSINT:pid}(:tid %{NUMBER:tid})?\\]( \\(%{POSINT:proxy_errorcode}\\)%{DATA:proxy_message}:)?( \\[client %{IPORHOST:clientip}:%{POSINT:clientport}\\])?( %{DATA:errorcode}:)? %{GREEDYDATA:message}"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"HTTPD_ERRORLOG"=>"%{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"COMMONAPACHELOG"=>"%{HTTPD_COMMONLOG}"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"COMBINEDAPACHELOG"=>"%{HTTPD_COMBINEDLOG}"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z$_][a-zA-Z$_0-9]*\\.)*[a-zA-Z$_][a-zA-Z$_0-9]*"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_. -]+)"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAMETHOD"=>"(?:(<(?:cl)?init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVASTACKTRACEPART"=>"%{SPACE}at %{JAVACLASS:class}\\.%{JAVAMETHOD:method}\\(%{JAVAFILE:file}(?::%{NUMBER:line})?\\)"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVATHREAD"=>"(?:[A-Z]{2}-Processor[\\d]+)"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z0-9-]+\\.)+[A-Za-z0-9$]+"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_.-]+)"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"JAVALOGMESSAGE"=>"(.*)"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"CATALINA_DATESTAMP"=>"%{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"TOMCAT_DATESTAMP"=>"20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"CATALINALOG"=>"%{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"TOMCATLOG"=>"%{TOMCAT_DATESTAMP:timestamp} \\| %{LOGLEVEL:level} \\| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW_EVENT"=>"(RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW1"=>"%{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \\d+\\(%{DATA:sent}\\) \\d+\\(%{DATA:received}\\) %{INT:elapsed-time} .*"}
[2020-01-30T22:49:50,239][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW2"=>"%{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"RT_FLOW3"=>"%{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\\(\\d\\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424PRINTASCII"=>"[!-~]+"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGBASE2"=>"(?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGPAMSESSION"=>"%{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\\(%{DATA:pam_caller}\\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"CRON_ACTION"=>"[A-Z ]+"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"CRONLOG"=>"%{SYSLOGBASE} \\(%{USER:user}\\) %{CRON_ACTION:action} \\(%{DATA:message}\\)"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOGLINE"=>"%{SYSLOGBASE2} %{GREEDYDATA:message}"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424PRI"=>"<%{NONNEGINT:syslog5424_pri}>"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424SD"=>"\\[%{DATA}\\]+"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424BASE"=>"%{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{IPORHOST:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"SYSLOG5424LINE"=>"%{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MAVEN_VERSION"=>"(?:(\\d+)\\.)?(?:(\\d+)\\.)?(\\*|\\d+)(?:[.-](RELEASE|SNAPSHOT))?"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVE"=>"., \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\]%{SPACE}%{LOGLEVEL:event_level}"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_LOG"=>"%{SYSLOGTIMESTAMP:timestamp} \\[%{WORD:component}\\] %{GREEDYDATA:message}"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_QUERY"=>"\\{ (?<={ ).*(?= } ntoreturn:) \\}"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_SLOWQUERY"=>"%{WORD} %{MONGO_WORDDASH:database}\\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO_WORDDASH"=>"\\b[\\w-]+\\b"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_SEVERITY"=>"\\w"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_COMPONENT"=>"%{WORD}|-"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"MONGO3_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\\[%{DATA:context}\\])? %{GREEDYDATA:message}"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOSTIME"=>"\\[%{NUMBER:nagios_epoch}\\]"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_CURRENT_SERVICE_STATE"=>"CURRENT SERVICE STATE"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_CURRENT_HOST_STATE"=>"CURRENT HOST STATE"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_NOTIFICATION"=>"SERVICE NOTIFICATION"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_NOTIFICATION"=>"HOST NOTIFICATION"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_ALERT"=>"SERVICE ALERT"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_ALERT"=>"HOST ALERT"}
[2020-01-30T22:49:50,254][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_FLAPPING_ALERT"=>"SERVICE FLAPPING ALERT"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_FLAPPING_ALERT"=>"HOST FLAPPING ALERT"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT"=>"SERVICE DOWNTIME ALERT"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_DOWNTIME_ALERT"=>"HOST DOWNTIME ALERT"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_PASSIVE_SERVICE_CHECK"=>"PASSIVE SERVICE CHECK"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_PASSIVE_HOST_CHECK"=>"PASSIVE HOST CHECK"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_SERVICE_EVENT_HANDLER"=>"SERVICE EVENT HANDLER"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_HOST_EVENT_HANDLER"=>"HOST EVENT HANDLER"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_EXTERNAL_COMMAND"=>"EXTERNAL COMMAND"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TYPE_TIMEPERIOD_TRANSITION"=>"TIMEPERIOD TRANSITION"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_SVC_CHECK"=>"DISABLE_SVC_CHECK"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_SVC_CHECK"=>"ENABLE_SVC_CHECK"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_CHECK"=>"DISABLE_HOST_CHECK"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_CHECK"=>"ENABLE_HOST_CHECK"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT"=>"PROCESS_SERVICE_CHECK_RESULT"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_PROCESS_HOST_CHECK_RESULT"=>"PROCESS_HOST_CHECK_RESULT"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME"=>"SCHEDULE_SERVICE_DOWNTIME"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_SCHEDULE_HOST_DOWNTIME"=>"SCHEDULE_HOST_DOWNTIME"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS"=>"DISABLE_HOST_SVC_NOTIFICATIONS"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS"=>"ENABLE_HOST_SVC_NOTIFICATIONS"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS"=>"DISABLE_HOST_NOTIFICATIONS"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS"=>"ENABLE_HOST_NOTIFICATIONS"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS"=>"DISABLE_SVC_NOTIFICATIONS"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS"=>"ENABLE_SVC_NOTIFICATIONS"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_WARNING"=>"Warning:%{SPACE}%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_CURRENT_SERVICE_STATE"=>"%{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_CURRENT_HOST_STATE"=>"%{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_NOTIFICATION"=>"%{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_NOTIFICATION"=>"%{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,270][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_ALERT"=>"%{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,286][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_ALERT"=>"%{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,286][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,286][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-01-30T22:49:50,286][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:49:50,286][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:49:50,286][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_PASSIVE_SERVICE_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:49:50,286][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_PASSIVE_HOST_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-01-30T22:49:50,286][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_SERVICE_EVENT_HANDLER"=>"%{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_HOST_EVENT_HANDLER"=>"%{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_TIMEPERIOD_TRANSITION"=>"%{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}"}
[2020-01-30T22:49:50,301][DEBUG][logstash.filters.grok    ] Adding pattern {"NAGIOSLOGLINE"=>"%{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"POSTGRESQL"=>"%{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RUUID"=>"\\h{32}"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RCONTROLLER"=>"(?<controller>[^#]+)#(?<action>\\w+)"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3HEAD"=>"(?m)Started %{WORD:verb} \"%{URIPATHPARAM:request}\" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RPROCESSING"=>"\\W*Processing by %{RCONTROLLER} as (?<format>\\S+)(?:\\W*Parameters: {%{DATA:params}}\\W*)?"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3FOOT"=>"Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3PROFILE"=>"(?:\\(Views: %{NUMBER:viewms}ms \\| ActiveRecord: %{NUMBER:activerecordms}ms|\\(ActiveRecord: %{NUMBER:activerecordms}ms)?"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RAILS3"=>"%{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\\n)*)(?:%{RAILS3FOOT})?"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISTIMESTAMP"=>"%{MONTHDAY} %{MONTH} %{TIME}"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISLOG"=>"\\[%{POSINT:pid}\\] %{REDISTIMESTAMP:timestamp} \\* "}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"REDISMONLOG"=>"%{NUMBER:timestamp} \\[%{INT:database} %{IP:client}:%{NUMBER:port}\\] \"%{WORD:command}\"\\s?%{GREEDYDATA:params}"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RUBY_LOGLEVEL"=>"(?:DEBUG|FATAL|ERROR|WARN|INFO)"}
[2020-01-30T22:49:50,317][DEBUG][logstash.filters.grok    ] Adding pattern {"RUBY_LOGGER"=>"[DFEWI], \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}"}
[2020-01-30T22:49:50,333][DEBUG][logstash.filters.grok    ] Adding pattern {"SQUID3"=>"%{NUMBER:timestamp}\\s+%{NUMBER:duration}\\s%{IP:client_address}\\s%{WORD:cache_result}/%{POSINT:status_code}\\s%{NUMBER:bytes}\\s%{WORD:request_method}\\s%{NOTSPACE:url}\\s(%{NOTSPACE:user}|-)\\s%{WORD:hierarchy_code}/%{IPORHOST:server}\\s%{NOTSPACE:content_type}"}
[2020-01-30T22:49:50,333][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<TIMESTAMP_ISO8601:timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?)
[2020-01-30T22:49:50,333][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?>\d\d){1,2})
[2020-01-30T22:49:50,333][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:0?[1-9]|1[0-2]))
[2020-01-30T22:49:50,333][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))
[2020-01-30T22:49:50,333][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-01-30T22:49:50,333][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-01-30T22:49:50,333][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?))
[2020-01-30T22:49:50,348][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:Z|[+-]%{HOUR}(?::?%{MINUTE})))
[2020-01-30T22:49:50,348][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-01-30T22:49:50,348][DEBUG][logstash.filters.grok    ] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-01-30T22:49:50,348][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<LOGLEVEL:severity>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?))
[2020-01-30T22:49:50,348][DEBUG][logstash.filters.grok    ] replacement_pattern => (?<GREEDYDATA:message>.*)
[2020-01-30T22:49:50,348][DEBUG][logstash.filters.grok    ] Grok compiled OK {:pattern=>"(?m)%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:severity} %{GREEDYDATA:message}", :expanded_pattern=>"(?m)(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01]?[0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?) (?<LOGLEVEL:severity>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)) (?<GREEDYDATA:message>.*)"}
[2020-01-30T22:49:50,348][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
[2020-01-30T22:49:50,834][INFO ][logstash.pipeline        ] Pipeline main started
[2020-01-30T22:49:50,850][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:49:50,850][DEBUG][logstash.inputs.file     ] _discover_file: D:/projects/elkstack/logstash-5.6.4/test.log: new: D:/projects/elkstack/logstash-5.6.4/test.log (exclude is [])
[2020-01-30T22:49:50,850][DEBUG][logstash.agent           ] Starting puma
[2020-01-30T22:49:50,865][DEBUG][logstash.inputs.file     ] _open_file: D:/projects/elkstack/logstash-5.6.4/test.log: opening
[2020-01-30T22:49:50,865][DEBUG][logstash.agent           ] Trying to start WebServer {:port=>9600}
[2020-01-30T22:49:50,865][DEBUG][logstash.inputs.file     ] D:/projects/elkstack/logstash-5.6.4/test.log: sincedb last value 1041, cur size 1041
[2020-01-30T22:49:50,865][DEBUG][logstash.inputs.file     ] D:/projects/elkstack/logstash-5.6.4/test.log: sincedb: seeking to 1041
[2020-01-30T22:49:50,865][DEBUG][logstash.api.service     ] [api-service] start
[2020-01-30T22:49:50,990][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-01-30T22:49:55,835][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:50:00,838][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:50:04,928][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:50:05,853][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:50:10,853][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:50:15,860][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:50:19,981][DEBUG][logstash.inputs.file     ] _globbed_files: D:/projects/elkstack/logstash-5.6.4/test.log: glob is: ["D:/projects/elkstack/logstash-5.6.4/test.log"]
[2020-01-30T22:50:20,750][WARN ][logstash.runner          ] SIGINT received. Shutting down the agent.
[2020-01-30T22:50:20,766][DEBUG][logstash.instrument.periodicpoller.os] PeriodicPoller: Stopping
[2020-01-30T22:50:20,766][DEBUG][logstash.instrument.periodicpoller.jvm] PeriodicPoller: Stopping
[2020-01-30T22:50:20,766][DEBUG][logstash.instrument.periodicpoller.persistentqueue] PeriodicPoller: Stopping
[2020-01-30T22:50:20,766][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] PeriodicPoller: Stopping
[2020-01-30T22:50:20,766][WARN ][logstash.agent           ] stopping pipeline {:id=>"main"}
[2020-01-30T22:50:20,782][DEBUG][logstash.pipeline        ] Closing inputs
[2020-01-30T22:50:20,782][DEBUG][logstash.inputs.file     ] stopping {:plugin=>"LogStash::Inputs::File"}
[2020-01-30T22:50:20,782][DEBUG][logstash.pipeline        ] Closed inputs
[2020-01-30T22:50:20,875][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:50:20,985][DEBUG][logstash.inputs.file     ] closing {:plugin=>"LogStash::Inputs::File"}
[2020-01-30T22:50:20,985][DEBUG][logstash.pipeline        ] Input plugins stopped! Will shutdown filter/output workers.
[2020-01-30T22:50:21,095][DEBUG][logstash.pipeline        ] Pushing flush onto pipeline
[2020-01-30T22:50:21,095][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x83648c6 run>"}
[2020-01-30T22:50:21,095][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x48081baf sleep>"}
[2020-01-30T22:50:21,095][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x2e25e6ab sleep>"}
[2020-01-30T22:50:21,095][DEBUG][logstash.pipeline        ] Pushing shutdown {:thread=>"#<Thread:0x7d4f69b8 sleep>"}
[2020-01-30T22:50:21,095][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x83648c6>
[2020-01-30T22:50:21,204][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x48081baf>
[2020-01-30T22:50:21,204][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x2e25e6ab>
[2020-01-30T22:50:21,204][DEBUG][logstash.pipeline        ] Shutdown waiting for worker thread #<Thread:0x7d4f69b8>
[2020-01-30T22:50:21,204][DEBUG][logstash.filters.mutate  ] closing {:plugin=>"LogStash::Filters::Mutate"}
[2020-01-30T22:50:21,204][DEBUG][logstash.filters.grok    ] closing {:plugin=>"LogStash::Filters::Grok"}
[2020-01-30T22:50:21,298][DEBUG][logstash.filters.date    ] closing {:plugin=>"LogStash::Filters::Date"}
[2020-01-30T22:50:21,298][DEBUG][logstash.outputs.stdout  ] closing {:plugin=>"LogStash::Outputs::Stdout"}
[2020-01-30T22:50:21,298][DEBUG][logstash.pipeline        ] Pipeline main has been shutdown
[2020-01-30T22:52:06,448][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration"}
[2020-01-30T22:52:06,464][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration"}
[2020-01-30T22:52:07,562][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
[2020-01-30T22:52:07,968][INFO ][logstash.pipeline        ] Pipeline main started
[2020-01-30T22:52:08,128][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-01-30T22:53:22,902][WARN ][logstash.runner          ] SIGINT received. Shutting down the agent.
[2020-01-30T22:53:22,918][WARN ][logstash.agent           ] stopping pipeline {:id=>"main"}
[2020-01-30T22:55:24,659][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"fb_apache", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/fb_apache/configuration"}
[2020-01-30T22:55:24,659][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>"netflow", :directory=>"D:/projects/elkstack/logstash-5.6.4/modules/netflow/configuration"}
[2020-01-30T22:55:25,788][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
[2020-01-30T22:55:26,178][INFO ][logstash.pipeline        ] Pipeline main started
[2020-01-30T22:55:26,350][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-01-30T22:55:38,266][WARN ][logstash.runner          ] SIGINT received. Shutting down the agent.
[2020-01-30T22:55:38,282][WARN ][logstash.agent           ] stopping pipeline {:id=>"main"}
